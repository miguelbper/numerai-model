{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerai API\n",
    "from numerapi import NumerAPI\n",
    "\n",
    "# data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# stats\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# other\n",
    "import gc\n",
    "import json\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# save variables\n",
    "import joblib\n",
    "\n",
    "# my utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 16:51:02,252 INFO numerapi.utils: target file already exists\n",
      "2022-08-14 16:51:02,254 INFO numerapi.utils: download complete\n"
     ]
    }
   ],
   "source": [
    "napi = NumerAPI()\n",
    "round = napi.get_current_round()\n",
    "era = round + 695\n",
    "\n",
    "# napi.download_dataset('v4/features.json', 'data/features.json')\n",
    "# napi.download_dataset('v4/train_int8.parquet', 'data/train.parquet')\n",
    "# napi.download_dataset('v4/validation_int8.parquet', 'data/validation.parquet')\n",
    "napi.download_dataset('v4/live_int8.parquet', f'data/live_{round}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'num_leaves': 2**5,\n",
    "    'colsample_bytree': 0.1,\n",
    "    'device': 'gpu',\n",
    "}\n",
    "\n",
    "x_cols = FEAT_L\n",
    "eras = None\n",
    "n_splits = 4\n",
    "\n",
    "df = read_data('train', x_cols=x_cols, eras=eras)\n",
    "X = df[x_cols]\n",
    "y = df[Y_TRUE]\n",
    "e = df[ERA]\n",
    "del df\n",
    "\n",
    "spl = TimeSeriesSplitGroups(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of era subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d175266fd24bb288df0896b4216866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "i:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b260b99267b40b08d906992f64ea154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0b79bdfed94c6f956df6308450f908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5e228211e8482a91fa686e14bd9b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26792c3f6fa841009f003cedadf0c0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc278607d3b344eab6f40d0de368265e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d1f64496084c838c368621a22d22da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b6723377a840e3a7fedd184947a5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293f7e14f63045c883b022214affba58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "j:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsamples = range(1, 9)\n",
    "corrs_nsubsamples = np.zeros((len(subsamples), n_splits))\n",
    "\n",
    "for i in tqdm(subsamples, desc='i', leave=False):\n",
    "    model = EraSubsampler(LGBMRegressor(**params), n_subsamples=i)\n",
    "    j = -1\n",
    "    for trn, val in tqdm(spl.split(X, y, e), desc='j', leave=False, total=n_splits):\n",
    "        j += 1\n",
    "        X_trn = X.iloc[trn]\n",
    "        X_val = X.iloc[val]\n",
    "        y_trn = y.iloc[trn]\n",
    "        y_val = y.iloc[val]\n",
    "        e_trn = e.iloc[trn]\n",
    "        e_val = e.iloc[val]\n",
    "\n",
    "        model.fit(X_trn, y_trn, eras=e_trn)\n",
    "\n",
    "        y_val_true = y_val\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        corrs_nsubsamples[i - 1, j] = corr(y_val_true, y_val_pred, rank_b=e_val)\n",
    "\n",
    "corrs_nsubsamples = pd.DataFrame(corrs_nsubsamples)\n",
    "corrs_nsubsamples['mean'] = corrs_nsubsamples.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took 145 min to run. Result:\n",
    "\n",
    "| n_subsamples |    split 1   |    split 2   |    split 3   |    split 4   |       mean   |\n",
    "|--------------|--------------|--------------|--------------|--------------|--------------|\n",
    "|           1  |   0.045875   |   0.053150   |   0.038182   |   0.051670   |   0.047219   |\n",
    "|           2  |   0.045747   |   0.053570   |   0.038385   |   0.051843   |   0.047386   |\n",
    "|           3  |   0.046136   |   0.054258   |   0.038141   |   0.051480   |   0.047504   |\n",
    "|           4  |   0.046392   |   0.054336   |   0.038378   |   0.051673   |   0.047695   |\n",
    "|           5  |   0.045795   |   0.053708   | **0.038695** |   0.051465   |   0.047416   |\n",
    "|           6  |   0.046698   |   0.054338   |   0.038311   |   0.051911   |   0.047815   |\n",
    "|           7  | **0.046976** | **0.054569** |   0.038347   | **0.052920** | **0.048203** |\n",
    "|           8  |   0.045762   |   0.053791   |   0.037597   |   0.052658   |   0.047452   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights for training on many targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of features to Neutralize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation (old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 1: Era / Feature subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'estimator__n_features_per_group': [0, 208],\n",
    "#     'n_subsamples': [1, 4],\n",
    "# }\n",
    "\n",
    "# model = LGBMRegressor(**params)\n",
    "# model = FeatureSubsampler(model, n_features_per_group=0)\n",
    "# model = EraSubsampler(model, n_subsamples=1)\n",
    "\n",
    "# gs = GridSearchCV(model, param_grid, cv=TimeSeriesSplitGroups(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df[ERA] = df[ERA].astype('int32')\n",
    "\n",
    "# X_COLS = FEAT_L\n",
    "\n",
    "# X = df[X_COLS]\n",
    "# y = df[Y_TRUE]\n",
    "# e = df[ERA]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# spl = TimeSeriesSplitGroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no feature subsampling, no era subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "\n",
    "# x_eras = np.arange(e.min(), e.max() + 1)\n",
    "# y_corr = np.zeros(e.max() + e.min() - 1, dtype=float)\n",
    "\n",
    "# e_val_min = []\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     print('\\tdefining X, y, e')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn] # ; print(f'e_trn_min = {e_trn.min()}, e_trn_max = {e_trn.max()}')\n",
    "#     e_val = e.iloc[val] # ; print(f'e_val_min = {e_val.min()}, e_val_max = {e_val.max()}')\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn)\n",
    "\n",
    "#     print('\\tcomputing predictions')\n",
    "#     # y_trn_pred = model.predict(X_trn)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     # corr_trn = corr(y_trn, y_trn_pred, rank_b=e_trn)\n",
    "#     corr_val = corr(y_val, y_val_pred, rank_b=e_val)\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y[e==era]\n",
    "#         y_era_pred = pd.DataFrame(e_val)\n",
    "#         y_era_pred['y_val_pred'] = y_val_pred\n",
    "#         y_era_pred = y_era_pred[e_val==era]\n",
    "#         y_era_pred = y_era_pred['y_val_pred']\n",
    "#         c = corr(y_era_true, y_era_pred)\n",
    "#         y_corr[era - 1] = c\n",
    "    \n",
    "#     e_val_min.append(e_val.min())\n",
    "#     c_val.append(corr_val)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# cr = np.mean(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.axhline(cr, color='green', linestyle='--')\n",
    "# ax.axhline(0, color='black', linewidth=1)\n",
    "# # ax.ytick\n",
    "# for e in e_val_min:\n",
    "#     ax.axvline(e, color='gray', linestyle='--')\n",
    "# ax.plot(x_eras, y_corr)\n",
    "# ax.set_xlabel('era')\n",
    "# ax.set_ylabel('corr')\n",
    "# ax.set_title(f'no feature or era subsampling. corr = {cr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no feature subsampling, with era subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "# model = EraSubsampler(model)\n",
    "\n",
    "# x_eras = np.arange(e.min(), e.max() + 1)\n",
    "# y_corr = np.zeros(e.max() + e.min() - 1, dtype=float)\n",
    "\n",
    "# e_val_min = []\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     print('\\tdefining X, y, e')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn] # ; print(f'e_trn_min = {e_trn.min()}, e_trn_max = {e_trn.max()}')\n",
    "#     e_val = e.iloc[val] # ; print(f'e_val_min = {e_val.min()}, e_val_max = {e_val.max()}')\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn, eras=e_trn)\n",
    "\n",
    "#     print('\\tcomputing predictions')\n",
    "#     # y_trn_pred = model.predict(X_trn)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     # corr_trn = corr(y_trn, y_trn_pred, rank_b=e_trn)\n",
    "#     corr_val = corr(y_val, y_val_pred, rank_b=e_val)\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y[e==era]\n",
    "#         y_era_pred = pd.DataFrame(e_val)\n",
    "#         y_era_pred['y_val_pred'] = y_val_pred\n",
    "#         y_era_pred = y_era_pred[e_val==era]\n",
    "#         y_era_pred = y_era_pred['y_val_pred']\n",
    "#         c = corr(y_era_true, y_era_pred)\n",
    "#         y_corr[era - 1] = c\n",
    "    \n",
    "#     e_val_min.append(e_val.min())\n",
    "#     c_val.append(corr_val)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# cr = np.mean(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.axhline(cr, color='green', linestyle='--')\n",
    "# ax.axhline(0, color='black', linewidth=1)\n",
    "# # ax.ytick\n",
    "# for e in e_val_min:\n",
    "#     ax.axvline(e, color='gray', linestyle='--')\n",
    "# ax.plot(x_eras, y_corr)\n",
    "# ax.set_xlabel('era')\n",
    "# ax.set_ylabel('corr')\n",
    "# ax.set_title(f'no feature or era subsampling. corr = {cr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with feature subsampling, no era subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "# model = FeatureSubsampler(model)\n",
    "\n",
    "# x_eras = np.arange(e.min(), e.max() + 1)\n",
    "# y_corr = np.zeros(e.max() + e.min() - 1, dtype=float)\n",
    "\n",
    "# e_val_min = []\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     print('\\tdefining X, y, e')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn] # ; print(f'e_trn_min = {e_trn.min()}, e_trn_max = {e_trn.max()}')\n",
    "#     e_val = e.iloc[val] # ; print(f'e_val_min = {e_val.min()}, e_val_max = {e_val.max()}')\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn)\n",
    "\n",
    "#     print('\\tcomputing predictions')\n",
    "#     # y_trn_pred = model.predict(X_trn)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     # corr_trn = corr(y_trn, y_trn_pred, rank_b=e_trn)\n",
    "#     corr_val = corr(y_val, y_val_pred, rank_b=e_val)\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y[e==era]\n",
    "#         y_era_pred = pd.DataFrame(e_val)\n",
    "#         y_era_pred['y_val_pred'] = y_val_pred\n",
    "#         y_era_pred = y_era_pred[e_val==era]\n",
    "#         y_era_pred = y_era_pred['y_val_pred']\n",
    "#         c = corr(y_era_true, y_era_pred)\n",
    "#         y_corr[era - 1] = c\n",
    "    \n",
    "#     e_val_min.append(e_val.min())\n",
    "#     c_val.append(corr_val)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# cr = np.mean(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.axhline(cr, color='green', linestyle='--')\n",
    "# ax.axhline(0, color='black', linewidth=1)\n",
    "# # ax.ytick\n",
    "# for e in e_val_min:\n",
    "#     ax.axvline(e, color='gray', linestyle='--')\n",
    "# ax.plot(x_eras, y_corr)\n",
    "# ax.set_xlabel('era')\n",
    "# ax.set_ylabel('corr')\n",
    "# ax.set_title(f'no feature or era subsampling. corr = {cr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with feature subsampling and era subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "# model = FeatureSubsampler(model)\n",
    "# model = EraSubsampler(model)\n",
    "\n",
    "# x_eras = np.arange(e.min(), e.max() + 1)\n",
    "# y_corr = np.zeros(e.max() + e.min() - 1, dtype=float)\n",
    "\n",
    "# e_val_min = []\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     print('\\tdefining X, y, e')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn] # ; print(f'e_trn_min = {e_trn.min()}, e_trn_max = {e_trn.max()}')\n",
    "#     e_val = e.iloc[val] # ; print(f'e_val_min = {e_val.min()}, e_val_max = {e_val.max()}')\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn, eras=e_trn)\n",
    "\n",
    "#     print('\\tcomputing predictions')\n",
    "#     # y_trn_pred = model.predict(X_trn)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     # corr_trn = corr(y_trn, y_trn_pred, rank_b=e_trn)\n",
    "#     corr_val = corr(y_val, y_val_pred, rank_b=e_val)\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y[e==era]\n",
    "#         y_era_pred = pd.DataFrame(e_val)\n",
    "#         y_era_pred['y_val_pred'] = y_val_pred\n",
    "#         y_era_pred = y_era_pred[e_val==era]\n",
    "#         y_era_pred = y_era_pred['y_val_pred']\n",
    "#         c = corr(y_era_true, y_era_pred)\n",
    "#         y_corr[era - 1] = c\n",
    "    \n",
    "#     e_val_min.append(e_val.min())\n",
    "#     c_val.append(corr_val)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# cr = np.mean(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.axhline(cr, color='green', linestyle='--')\n",
    "# ax.axhline(0, color='black', linewidth=1)\n",
    "# # ax.ytick\n",
    "# for e in e_val_min:\n",
    "#     ax.axvline(e, color='gray', linestyle='--')\n",
    "# ax.plot(x_eras, y_corr)\n",
    "# ax.set_xlabel('era')\n",
    "# ax.set_ylabel('corr')\n",
    "# ax.set_title(f'no feature or era subsampling. corr = {cr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 2: targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### just average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# df = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df[ERA] = df[ERA].astype('int32')\n",
    "# df = df.fillna(0.5)\n",
    "\n",
    "# X_COLS = FEAT_L\n",
    "\n",
    "# X = df[X_COLS]\n",
    "# y = df[Y_COLS]\n",
    "# e = df[ERA]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# spl = TimeSeriesSplitGroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = LGBMRegressor(**params)\n",
    "# # model = EraSubsampler(model)\n",
    "# # model = MultiTargetTrainer(model)\n",
    "\n",
    "# from itertools import chain, combinations\n",
    "\n",
    "# def nempty_subsets(iterable):\n",
    "#     s = list(iterable)\n",
    "#     return chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))\n",
    "\n",
    "# corr_dict = {\n",
    "#     'subset': [],\n",
    "#     'fold_1': [],\n",
    "#     'fold_2': [],\n",
    "#     'fold_3': [],\n",
    "#     'fold_4': [],\n",
    "#     'fold_5': [],\n",
    "# }\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_val = y[Y_TRUE].iloc[val]\n",
    "#     e_val = e.iloc[val]\n",
    "#     # X_trn = X.iloc[trn]\n",
    "#     # y_trn = y.iloc[trn]\n",
    "#     # e_trn = e.iloc[trn]\n",
    "\n",
    "#     # print('\\ttraining model')\n",
    "#     # model.fit(X_trn, y_trn, eras=e_trn)\n",
    "#     # joblib.dump(model, f'model-0/saved-variables/multi_target_fold_{i}.pkl')\n",
    "#     model = joblib.load(f'model-0/saved-variables/multi_target_fold_{i}.pkl')\n",
    "\n",
    "#     # print('\\tcomputing predictions')\n",
    "#     # y_val_pred = model.model.predict(X_val)\n",
    "#     # joblib.dump(y_val_pred, f'model-0/saved-variables/y_val_pred_{i}.pkl')\n",
    "#     y_val_pred = joblib.load(f'model-0/saved-variables/y_val_pred_{i}.pkl')\n",
    "\n",
    "#     j = 0\n",
    "#     for subset in nempty_subsets(range(10)):\n",
    "#         j += 1\n",
    "#         print(f'\\r\\tin iteration {j}/1023 of subsets', \n",
    "#               end=' '*len('        in iteration 1023/1023 of subsets'))\n",
    "\n",
    "#         y_pred = np.average(y_val_pred[:, subset], axis=1)\n",
    "\n",
    "#         c = corr(y_val, y_pred, rank_b=e_val)\n",
    "\n",
    "#         corr_dict[f'fold_{i}'].append(c)\n",
    "#         if i == 1:\n",
    "#             corr_dict['subset'].append(subset)\n",
    "\n",
    "# corr_df = pd.DataFrame(corr_dict)\n",
    "# corr_df['avg'] = corr_df[['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']].mean(1)\n",
    "# joblib.dump(corr_df, 'model-0/saved-variables/corr_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternative: train linear model on (predictions of multitarget, true)\n",
    "# # see if that generalizes better out of sample\n",
    "\n",
    "# # model = LGBMRegressor(**params)\n",
    "# # model = EraSubsampler(model)\n",
    "# # model = MultiTargetTrainer(model)\n",
    "\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     X_val = X.iloc[val]\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     e_val = e.iloc[val]\n",
    "#     e_trn = e.iloc[trn]\n",
    "\n",
    "#     # print('\\ttraining model')\n",
    "#     # model.fit(X_trn, y_trn, eras=e_trn)\n",
    "#     # joblib.dump(model, f'model-0/saved-variables/multi_target_fold_{i}.pkl')\n",
    "#     model = joblib.load(f'model-0/saved-variables/multi_target_fold_{i}.pkl')\n",
    "    \n",
    "#     print('\\tpredicting Y on training set')\n",
    "#     y_trn_pred = model.predict(X_trn)\n",
    "#     joblib.dump(y_trn_pred, f'model-0/saved-variables/y_trn_pred_{i}.pkl')\n",
    "\n",
    "#     print('\\ttraining linear regression on (Y_trn_pred, y_trn_true)')\n",
    "#     aux_lin = LinearRegression()\n",
    "#     aux_lin.fit(y_trn_pred, y_trn[Y_TRUE])\n",
    "#     joblib.dump(aux_lin, f'model-0/saved-variables/aux_lin_{i}.pkl')\n",
    "\n",
    "#     print('\\tpredicting with linear regression & computing corr')\n",
    "#     y_val_pred = joblib.load(f'model-0/saved-variables/y_val_pred_{i}.pkl')\n",
    "#     y_val_pred_lr = aux_lin.predict(y_val_pred)\n",
    "\n",
    "#     c = corr(y_val, y_val_pred_lr, rank_b=e_val)\n",
    "#     c_val.append(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# df = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df[ERA] = df[ERA].astype('int32')\n",
    "# df = df.fillna(0.5)\n",
    "# df = df[df[ERA].isin(np.arange(1, 574, 4))]\n",
    "\n",
    "# X_COLS = FEAT_L\n",
    "\n",
    "# X = df[X_COLS]\n",
    "# y = df[Y_COLS]\n",
    "# e = df[ERA]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# spl = TimeSeriesSplitGroups(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "# model = MultiOutputRegressor(model)\n",
    "\n",
    "# corr_dict_fold = {\n",
    "#     'corr_one': [],\n",
    "#     'corr_lin': [],\n",
    "#     'corr_lgb': [],\n",
    "# }\n",
    "\n",
    "# eras = np.arange(e.min(), e.max() + 1)\n",
    "\n",
    "# corr_dict_era = {\n",
    "#     'corr_one': np.zeros(e.max() + e.min() - 1, dtype=float),\n",
    "#     'corr_lin': np.zeros(e.max() + e.min() - 1, dtype=float),\n",
    "#     'corr_lgb': np.zeros(e.max() + e.min() - 1, dtype=float),\n",
    "# }\n",
    "\n",
    "# e_val_min = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     Y_trn = y.iloc[trn]\n",
    "#     Y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn]\n",
    "#     e_val = e.iloc[val]\n",
    "#     y_trn = Y_trn[Y_TRUE]\n",
    "#     y_val = Y_val[Y_TRUE]\n",
    "\n",
    "#     e_val_min.append(e_val.min())\n",
    "\n",
    "#     print('\\tmulti output')\n",
    "#     print('\\t\\ttraining')\n",
    "#     model.fit(X_trn, Y_trn)\n",
    "#     print('\\t\\tpredicting on train')\n",
    "#     Y_trn_pred = model.predict(X_trn)\n",
    "#     print('\\t\\tpredicting on validation')\n",
    "#     Y_val_pred = model.predict(X_val)\n",
    "#     y_val_pred_one = Y_val_pred[:, 0]\n",
    "\n",
    "#     print('\\tlinear model')\n",
    "#     aux_lin = LinearRegression()\n",
    "#     print('\\t\\ttraining')\n",
    "#     aux_lin.fit(Y_trn_pred, y_trn)\n",
    "#     print('\\t\\tpredicting')\n",
    "#     y_val_pred_lin = aux_lin.predict(Y_val_pred)\n",
    "\n",
    "#     print('\\trandom forest')\n",
    "#     aux_lgb = LGBMRegressor(**params)\n",
    "#     print('\\t\\ttraining')\n",
    "#     aux_lgb.fit(Y_trn_pred, y_trn)\n",
    "#     print('\\t\\tpredicting')\n",
    "#     y_val_pred_lgb = aux_lgb.predict(Y_val_pred)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     corr_dict_fold['corr_one'].append(corr(y_val, y_val_pred_one, rank_b=e_val))\n",
    "#     corr_dict_fold['corr_lin'].append(corr(y_val, y_val_pred_lin, rank_b=e_val))\n",
    "#     corr_dict_fold['corr_lgb'].append(corr(y_val, y_val_pred_lgb, rank_b=e_val))\n",
    "\n",
    "#     aux_df = pd.DataFrame(e_val)\n",
    "#     aux_df['y_val_pred_one'] = y_val_pred_one\n",
    "#     aux_df['y_val_pred_lin'] = y_val_pred_lin\n",
    "#     aux_df['y_val_pred_lgb'] = y_val_pred_lgb\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y_val[e_val==era]\n",
    "#         aux_df_era = aux_df[e_val==era]\n",
    "#         y_era_pred_one = aux_df_era['y_val_pred_one'] #\n",
    "#         y_era_pred_lin = aux_df_era['y_val_pred_lin'] #\n",
    "#         y_era_pred_lgb = aux_df_era['y_val_pred_lgb'] #\n",
    "#         corr_dict_era['corr_one'][era - 1] = corr(y_era_true, y_era_pred_one)\n",
    "#         corr_dict_era['corr_lin'][era - 1] = corr(y_era_true, y_era_pred_lin)\n",
    "#         corr_dict_era['corr_lgb'][era - 1] = corr(y_era_true, y_era_pred_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(corr_dict_era, 'model-0/saved-variables/corr_dict_era.pkl')\n",
    "# joblib.dump(corr_dict_fold, 'model-0/saved-variables/corr_dict_fold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(25, 5))\n",
    "# ax1.axhline(0, color='black', linewidth=1)\n",
    "# ax2.axhline(0, color='black', linewidth=1)\n",
    "# ax3.axhline(0, color='black', linewidth=1)\n",
    "# for e in e_val_min:\n",
    "#     ax1.axvline(e, color='gray', linestyle='--')\n",
    "#     ax2.axvline(e, color='gray', linestyle='--')\n",
    "#     ax3.axvline(e, color='gray', linestyle='--')\n",
    "# ax1.plot(eras, corr_dict_era['corr_one'], color='blue'  , linewidth=1, label='one feature')\n",
    "# ax2.plot(eras, corr_dict_era['corr_lin'], color='orange', linewidth=1, label='linear ensemble')\n",
    "# ax3.plot(eras, corr_dict_era['corr_lgb'], color='green' , linewidth=1, label='forest ensemble')\n",
    "# ax1.set_ylim(-0.05, 0.15)\n",
    "# ax2.set_ylim(-0.05, 0.15)\n",
    "# ax3.set_ylim(-0.05, 0.15)\n",
    "# ax1.set_xlabel('era')\n",
    "# ax1.set_ylabel('corr')\n",
    "# ax1.set_title(f'train on many targets and ensemble')\n",
    "# ax1.legend()\n",
    "# ax2.set_xlabel('era')\n",
    "# ax2.set_ylabel('corr')\n",
    "# ax2.set_title(f'train on many targets and ensemble')\n",
    "# ax2.legend()\n",
    "# ax3.set_xlabel('era')\n",
    "# ax3.set_ylabel('corr')\n",
    "# ax3.set_title(f'train on many targets and ensemble')\n",
    "# ax3.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_df_fold = pd.DataFrame(corr_dict_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 3: Feature neutralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# df = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df[ERA] = df[ERA].astype('int32')\n",
    "# df = df.fillna(0.5)\n",
    "\n",
    "# X_COLS = FEAT_L\n",
    "\n",
    "# X = df[X_COLS]\n",
    "# y = df[Y_TRUE]\n",
    "# e = df[ERA]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# spl = TimeSeriesSplitGroups()\n",
    "\n",
    "# model = LGBMRegressor(**params)\n",
    "# model = EraSubsampler(model)\n",
    "# model = FeatureNeutralizer(model, n_features=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_numbers = [5, 10, 25, 50, 100]\n",
    "# alphas = np.arange(0, 1, 0.01)\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     X_val = X.iloc[val]\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     e_val = e.iloc[val]\n",
    "#     e_trn = e.iloc[trn]\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn, eras=e_trn)\n",
    "#     joblib.dump(model, f'model-0/saved-variables/model_neut_{i}')\n",
    "\n",
    "#     print('\\tcomputing y_pred')\n",
    "#     model.compute_y_pred(X_val)\n",
    "#     joblib.dump(model, f'model-0/saved-variables/model_neut_{i}')\n",
    "\n",
    "#     print('\\tcomputing corrs')\n",
    "#     for n_feats in feat_numbers:\n",
    "#         model.set_params(n_features=n_feats)\n",
    "#         model.compute_y_linr(X_val, groups=e_val)\n",
    "#         corrs = [(corr(y_val, model.y_pred - a * model.y_linr, rank_a=e_val), a) for a in alphas]\n",
    "#         c, a = sorted(corrs, reverse=True)[0]\n",
    "#         print(f'\\t\\tCV_fold = {i}, n_features = {n_feats}, a = {a:.2f}, c = {c:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# model = LGBMRegressor(**params)\n",
    "# model = EraSubsampler(model)\n",
    "\n",
    "# df_trn = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df_trn[ERA] = df_trn[ERA].astype('int32')\n",
    "\n",
    "# model.fit(df_trn[X_COLS], df_trn[Y_TRUE], eras=df_trn[ERA])\n",
    "\n",
    "# joblib.dump(model, f'model-0/saved-variables/lgbm_{now_dt()}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_liv = pd.read_parquet(f'data/live_{round}.parquet', columns=COLUMNS)\n",
    "\n",
    "# model = joblib.load('model-0/saved-variables/lgbm_2022-08-07-09-37.pkl')\n",
    "# df_liv[Y_PRED] = model.predict(df_liv[X_COLS])\n",
    "# df_liv[Y_RANK] = df_liv[Y_PRED].rank(pct=True)\n",
    "# df_liv[Y_RANK].to_csv(f'model-0/predictions/lgbm_live_predictions_{round}_{now_dt()}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d202d1b6c0c7975210c24a4862339e0f7f90d66cb89735f264f6c4d5c4350e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
