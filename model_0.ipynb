{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerai API\n",
    "from numerapi import NumerAPI\n",
    "\n",
    "# data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# stats\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# other\n",
    "import gc\n",
    "import json\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# save variables\n",
    "import joblib\n",
    "\n",
    "# my utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 09:44:33,549 INFO numerapi.utils: target file already exists\n",
      "2022-08-07 09:44:33,549 INFO numerapi.utils: download complete\n",
      "2022-08-07 09:44:34,561 INFO numerapi.utils: target file already exists\n",
      "2022-08-07 09:44:34,563 INFO numerapi.utils: download complete\n",
      "2022-08-07 09:44:35,573 INFO numerapi.utils: target file already exists\n",
      "2022-08-07 09:44:35,574 INFO numerapi.utils: download complete\n",
      "2022-08-07 09:44:36,499 INFO numerapi.utils: target file already exists\n",
      "2022-08-07 09:44:36,502 INFO numerapi.utils: download complete\n"
     ]
    }
   ],
   "source": [
    "napi = NumerAPI()\n",
    "round = napi.get_current_round()\n",
    "era = round + 695\n",
    "\n",
    "napi.download_dataset('v4/features.json', 'data/features.json')\n",
    "napi.download_dataset('v4/train_int8.parquet', 'data/train.parquet')\n",
    "napi.download_dataset('v4/validation_int8.parquet', 'data/validation.parquet')\n",
    "napi.download_dataset('v4/live_int8.parquet', f'data/live_{round}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV 1: Era / Feature subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'estimator__n_features_per_group': [0, 208],\n",
    "#     'n_subsamples': [1, 4],\n",
    "# }\n",
    "\n",
    "# model = LGBMRegressor(**params)\n",
    "# model = FeatureSubsampler(model, n_features_per_group=0)\n",
    "# model = EraSubsampler(model, n_subsamples=1)\n",
    "\n",
    "# gs = GridSearchCV(model, param_grid, cv=TimeSeriesSplitGroups(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df[ERA] = df[ERA].astype('int32')\n",
    "\n",
    "# X_COLS = FEAT_L\n",
    "\n",
    "# X = df[X_COLS]\n",
    "# y = df[Y_TRUE]\n",
    "# e = df[ERA]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# spl = TimeSeriesSplitGroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no feature subsampling, no era subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "\n",
    "# x_eras = np.arange(e.min(), e.max() + 1)\n",
    "# y_corr = np.zeros(e.max() + e.min() - 1, dtype=float)\n",
    "\n",
    "# e_val_min = []\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     print('\\tdefining X, y, e')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn] # ; print(f'e_trn_min = {e_trn.min()}, e_trn_max = {e_trn.max()}')\n",
    "#     e_val = e.iloc[val] # ; print(f'e_val_min = {e_val.min()}, e_val_max = {e_val.max()}')\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn)\n",
    "\n",
    "#     print('\\tcomputing predictions')\n",
    "#     # y_trn_pred = model.predict(X_trn)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     # corr_trn = corr(y_trn, y_trn_pred, rank_b=e_trn)\n",
    "#     corr_val = corr(y_val, y_val_pred, rank_b=e_val)\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y[e==era]\n",
    "#         y_era_pred = pd.DataFrame(e_val)\n",
    "#         y_era_pred['y_val_pred'] = y_val_pred\n",
    "#         y_era_pred = y_era_pred[e_val==era]\n",
    "#         y_era_pred = y_era_pred['y_val_pred']\n",
    "#         c = corr(y_era_true, y_era_pred)\n",
    "#         y_corr[era - 1] = c\n",
    "    \n",
    "#     e_val_min.append(e_val.min())\n",
    "#     c_val.append(corr_val)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# cr = np.mean(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.axhline(cr, color='green', linestyle='--')\n",
    "# ax.axhline(0, color='black', linewidth=1)\n",
    "# # ax.ytick\n",
    "# for e in e_val_min:\n",
    "#     ax.axvline(e, color='gray', linestyle='--')\n",
    "# ax.plot(x_eras, y_corr)\n",
    "# ax.set_xlabel('era')\n",
    "# ax.set_ylabel('corr')\n",
    "# ax.set_title(f'no feature or era subsampling. corr = {cr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no feature subsampling, with era subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "# model = EraSubsampler(model)\n",
    "\n",
    "# x_eras = np.arange(e.min(), e.max() + 1)\n",
    "# y_corr = np.zeros(e.max() + e.min() - 1, dtype=float)\n",
    "\n",
    "# e_val_min = []\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     print('\\tdefining X, y, e')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn] # ; print(f'e_trn_min = {e_trn.min()}, e_trn_max = {e_trn.max()}')\n",
    "#     e_val = e.iloc[val] # ; print(f'e_val_min = {e_val.min()}, e_val_max = {e_val.max()}')\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn, eras=e_trn)\n",
    "\n",
    "#     print('\\tcomputing predictions')\n",
    "#     # y_trn_pred = model.predict(X_trn)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     # corr_trn = corr(y_trn, y_trn_pred, rank_b=e_trn)\n",
    "#     corr_val = corr(y_val, y_val_pred, rank_b=e_val)\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y[e==era]\n",
    "#         y_era_pred = pd.DataFrame(e_val)\n",
    "#         y_era_pred['y_val_pred'] = y_val_pred\n",
    "#         y_era_pred = y_era_pred[e_val==era]\n",
    "#         y_era_pred = y_era_pred['y_val_pred']\n",
    "#         c = corr(y_era_true, y_era_pred)\n",
    "#         y_corr[era - 1] = c\n",
    "    \n",
    "#     e_val_min.append(e_val.min())\n",
    "#     c_val.append(corr_val)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# cr = np.mean(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.axhline(cr, color='green', linestyle='--')\n",
    "# ax.axhline(0, color='black', linewidth=1)\n",
    "# # ax.ytick\n",
    "# for e in e_val_min:\n",
    "#     ax.axvline(e, color='gray', linestyle='--')\n",
    "# ax.plot(x_eras, y_corr)\n",
    "# ax.set_xlabel('era')\n",
    "# ax.set_ylabel('corr')\n",
    "# ax.set_title(f'no feature or era subsampling. corr = {cr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with feature subsampling, no era subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "# model = FeatureSubsampler(model)\n",
    "\n",
    "# x_eras = np.arange(e.min(), e.max() + 1)\n",
    "# y_corr = np.zeros(e.max() + e.min() - 1, dtype=float)\n",
    "\n",
    "# e_val_min = []\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     print('\\tdefining X, y, e')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn] # ; print(f'e_trn_min = {e_trn.min()}, e_trn_max = {e_trn.max()}')\n",
    "#     e_val = e.iloc[val] # ; print(f'e_val_min = {e_val.min()}, e_val_max = {e_val.max()}')\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn)\n",
    "\n",
    "#     print('\\tcomputing predictions')\n",
    "#     # y_trn_pred = model.predict(X_trn)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     # corr_trn = corr(y_trn, y_trn_pred, rank_b=e_trn)\n",
    "#     corr_val = corr(y_val, y_val_pred, rank_b=e_val)\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y[e==era]\n",
    "#         y_era_pred = pd.DataFrame(e_val)\n",
    "#         y_era_pred['y_val_pred'] = y_val_pred\n",
    "#         y_era_pred = y_era_pred[e_val==era]\n",
    "#         y_era_pred = y_era_pred['y_val_pred']\n",
    "#         c = corr(y_era_true, y_era_pred)\n",
    "#         y_corr[era - 1] = c\n",
    "    \n",
    "#     e_val_min.append(e_val.min())\n",
    "#     c_val.append(corr_val)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# cr = np.mean(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.axhline(cr, color='green', linestyle='--')\n",
    "# ax.axhline(0, color='black', linewidth=1)\n",
    "# # ax.ytick\n",
    "# for e in e_val_min:\n",
    "#     ax.axvline(e, color='gray', linestyle='--')\n",
    "# ax.plot(x_eras, y_corr)\n",
    "# ax.set_xlabel('era')\n",
    "# ax.set_ylabel('corr')\n",
    "# ax.set_title(f'no feature or era subsampling. corr = {cr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with feature subsampling and era subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "# model = FeatureSubsampler(model)\n",
    "# model = EraSubsampler(model)\n",
    "\n",
    "# x_eras = np.arange(e.min(), e.max() + 1)\n",
    "# y_corr = np.zeros(e.max() + e.min() - 1, dtype=float)\n",
    "\n",
    "# e_val_min = []\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     print('\\tdefining X, y, e')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn] # ; print(f'e_trn_min = {e_trn.min()}, e_trn_max = {e_trn.max()}')\n",
    "#     e_val = e.iloc[val] # ; print(f'e_val_min = {e_val.min()}, e_val_max = {e_val.max()}')\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn, eras=e_trn)\n",
    "\n",
    "#     print('\\tcomputing predictions')\n",
    "#     # y_trn_pred = model.predict(X_trn)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     # corr_trn = corr(y_trn, y_trn_pred, rank_b=e_trn)\n",
    "#     corr_val = corr(y_val, y_val_pred, rank_b=e_val)\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y[e==era]\n",
    "#         y_era_pred = pd.DataFrame(e_val)\n",
    "#         y_era_pred['y_val_pred'] = y_val_pred\n",
    "#         y_era_pred = y_era_pred[e_val==era]\n",
    "#         y_era_pred = y_era_pred['y_val_pred']\n",
    "#         c = corr(y_era_true, y_era_pred)\n",
    "#         y_corr[era - 1] = c\n",
    "    \n",
    "#     e_val_min.append(e_val.min())\n",
    "#     c_val.append(corr_val)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# cr = np.mean(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.axhline(cr, color='green', linestyle='--')\n",
    "# ax.axhline(0, color='black', linewidth=1)\n",
    "# # ax.ytick\n",
    "# for e in e_val_min:\n",
    "#     ax.axvline(e, color='gray', linestyle='--')\n",
    "# ax.plot(x_eras, y_corr)\n",
    "# ax.set_xlabel('era')\n",
    "# ax.set_ylabel('corr')\n",
    "# ax.set_title(f'no feature or era subsampling. corr = {cr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV 2: targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# df = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df[ERA] = df[ERA].astype('int32')\n",
    "# df = df.fillna(0.5)\n",
    "\n",
    "# X_COLS = FEAT_L\n",
    "\n",
    "# X = df[X_COLS]\n",
    "# y = df[Y_COLS]\n",
    "# e = df[ERA]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# spl = TimeSeriesSplitGroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = LGBMRegressor(**params)\n",
    "# # model = EraSubsampler(model)\n",
    "# # model = MultiTargetTrainer(model)\n",
    "\n",
    "# from itertools import chain, combinations\n",
    "\n",
    "# def nempty_subsets(iterable):\n",
    "#     s = list(iterable)\n",
    "#     return chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))\n",
    "\n",
    "# corr_dict = {\n",
    "#     'subset': [],\n",
    "#     'fold_1': [],\n",
    "#     'fold_2': [],\n",
    "#     'fold_3': [],\n",
    "#     'fold_4': [],\n",
    "#     'fold_5': [],\n",
    "# }\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     X_val = X.iloc[val]\n",
    "#     y_val = y[Y_TRUE].iloc[val]\n",
    "#     e_val = e.iloc[val]\n",
    "#     # X_trn = X.iloc[trn]\n",
    "#     # y_trn = y.iloc[trn]\n",
    "#     # e_trn = e.iloc[trn]\n",
    "\n",
    "#     # print('\\ttraining model')\n",
    "#     # model.fit(X_trn, y_trn, eras=e_trn)\n",
    "#     # joblib.dump(model, f'model-0/saved-variables/multi_target_fold_{i}.pkl')\n",
    "#     model = joblib.load(f'model-0/saved-variables/multi_target_fold_{i}.pkl')\n",
    "\n",
    "#     # print('\\tcomputing predictions')\n",
    "#     # y_val_pred = model.model.predict(X_val)\n",
    "#     # joblib.dump(y_val_pred, f'model-0/saved-variables/y_val_pred_{i}.pkl')\n",
    "#     y_val_pred = joblib.load(f'model-0/saved-variables/y_val_pred_{i}.pkl')\n",
    "\n",
    "#     j = 0\n",
    "#     for subset in nempty_subsets(range(10)):\n",
    "#         j += 1\n",
    "#         print(f'\\r\\tin iteration {j}/1023 of subsets', \n",
    "#               end=' '*len('        in iteration 1023/1023 of subsets'))\n",
    "\n",
    "#         y_pred = np.average(y_val_pred[:, subset], axis=1)\n",
    "\n",
    "#         c = corr(y_val, y_pred, rank_b=e_val)\n",
    "\n",
    "#         corr_dict[f'fold_{i}'].append(c)\n",
    "#         if i == 1:\n",
    "#             corr_dict['subset'].append(subset)\n",
    "\n",
    "# corr_df = pd.DataFrame(corr_dict)\n",
    "# corr_df['avg'] = corr_df[['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']].mean(1)\n",
    "# joblib.dump(corr_df, 'model-0/saved-variables/corr_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternative: train linear model on (predictions of multitarget, true)\n",
    "# # see if that generalizes better out of sample\n",
    "\n",
    "# # model = LGBMRegressor(**params)\n",
    "# # model = EraSubsampler(model)\n",
    "# # model = MultiTargetTrainer(model)\n",
    "\n",
    "# c_val = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     X_val = X.iloc[val]\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     e_val = e.iloc[val]\n",
    "#     e_trn = e.iloc[trn]\n",
    "\n",
    "#     # print('\\ttraining model')\n",
    "#     # model.fit(X_trn, y_trn, eras=e_trn)\n",
    "#     # joblib.dump(model, f'model-0/saved-variables/multi_target_fold_{i}.pkl')\n",
    "#     model = joblib.load(f'model-0/saved-variables/multi_target_fold_{i}.pkl')\n",
    "    \n",
    "#     print('\\tpredicting Y on training set')\n",
    "#     y_trn_pred = model.predict(X_trn)\n",
    "#     joblib.dump(y_trn_pred, f'model-0/saved-variables/y_trn_pred_{i}.pkl')\n",
    "\n",
    "#     print('\\ttraining linear regression on (Y_trn_pred, y_trn_true)')\n",
    "#     aux_lin = LinearRegression()\n",
    "#     aux_lin.fit(y_trn_pred, y_trn[Y_TRUE])\n",
    "#     joblib.dump(aux_lin, f'model-0/saved-variables/aux_lin_{i}.pkl')\n",
    "\n",
    "#     print('\\tpredicting with linear regression & computing corr')\n",
    "#     y_val_pred = joblib.load(f'model-0/saved-variables/y_val_pred_{i}.pkl')\n",
    "#     y_val_pred_lr = aux_lin.predict(y_val_pred)\n",
    "\n",
    "#     c = corr(y_val, y_val_pred_lr, rank_b=e_val)\n",
    "#     c_val.append(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# df = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df[ERA] = df[ERA].astype('int32')\n",
    "# df = df.fillna(0.5)\n",
    "# df = df[df[ERA].isin(np.arange(1, 574, 4))]\n",
    "\n",
    "# X_COLS = FEAT_L\n",
    "\n",
    "# X = df[X_COLS]\n",
    "# y = df[Y_COLS]\n",
    "# e = df[ERA]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# spl = TimeSeriesSplitGroups(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(**params)\n",
    "# model = MultiOutputRegressor(model)\n",
    "\n",
    "# corr_dict_fold = {\n",
    "#     'corr_one': [],\n",
    "#     'corr_lin': [],\n",
    "#     'corr_lgb': [],\n",
    "# }\n",
    "\n",
    "# eras = np.arange(e.min(), e.max() + 1)\n",
    "\n",
    "# corr_dict_era = {\n",
    "#     'corr_one': np.zeros(e.max() + e.min() - 1, dtype=float),\n",
    "#     'corr_lin': np.zeros(e.max() + e.min() - 1, dtype=float),\n",
    "#     'corr_lgb': np.zeros(e.max() + e.min() - 1, dtype=float),\n",
    "# }\n",
    "\n",
    "# e_val_min = []\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     X_val = X.iloc[val]\n",
    "#     Y_trn = y.iloc[trn]\n",
    "#     Y_val = y.iloc[val]\n",
    "#     e_trn = e.iloc[trn]\n",
    "#     e_val = e.iloc[val]\n",
    "#     y_trn = Y_trn[Y_TRUE]\n",
    "#     y_val = Y_val[Y_TRUE]\n",
    "\n",
    "#     e_val_min.append(e_val.min())\n",
    "\n",
    "#     print('\\tmulti output')\n",
    "#     print('\\t\\ttraining')\n",
    "#     model.fit(X_trn, Y_trn)\n",
    "#     print('\\t\\tpredicting on train')\n",
    "#     Y_trn_pred = model.predict(X_trn)\n",
    "#     print('\\t\\tpredicting on validation')\n",
    "#     Y_val_pred = model.predict(X_val)\n",
    "#     y_val_pred_one = Y_val_pred[:, 0]\n",
    "\n",
    "#     print('\\tlinear model')\n",
    "#     aux_lin = LinearRegression()\n",
    "#     print('\\t\\ttraining')\n",
    "#     aux_lin.fit(Y_trn_pred, y_trn)\n",
    "#     print('\\t\\tpredicting')\n",
    "#     y_val_pred_lin = aux_lin.predict(Y_val_pred)\n",
    "\n",
    "#     print('\\trandom forest')\n",
    "#     aux_lgb = LGBMRegressor(**params)\n",
    "#     print('\\t\\ttraining')\n",
    "#     aux_lgb.fit(Y_trn_pred, y_trn)\n",
    "#     print('\\t\\tpredicting')\n",
    "#     y_val_pred_lgb = aux_lgb.predict(Y_val_pred)\n",
    "\n",
    "#     print('\\tcomputing correlations')\n",
    "#     corr_dict_fold['corr_one'].append(corr(y_val, y_val_pred_one, rank_b=e_val))\n",
    "#     corr_dict_fold['corr_lin'].append(corr(y_val, y_val_pred_lin, rank_b=e_val))\n",
    "#     corr_dict_fold['corr_lgb'].append(corr(y_val, y_val_pred_lgb, rank_b=e_val))\n",
    "\n",
    "#     aux_df = pd.DataFrame(e_val)\n",
    "#     aux_df['y_val_pred_one'] = y_val_pred_one\n",
    "#     aux_df['y_val_pred_lin'] = y_val_pred_lin\n",
    "#     aux_df['y_val_pred_lgb'] = y_val_pred_lgb\n",
    "\n",
    "#     for era in e_val.unique():\n",
    "#         y_era_true = y_val[e_val==era]\n",
    "#         aux_df_era = aux_df[e_val==era]\n",
    "#         y_era_pred_one = aux_df_era['y_val_pred_one'] #\n",
    "#         y_era_pred_lin = aux_df_era['y_val_pred_lin'] #\n",
    "#         y_era_pred_lgb = aux_df_era['y_val_pred_lgb'] #\n",
    "#         corr_dict_era['corr_one'][era - 1] = corr(y_era_true, y_era_pred_one)\n",
    "#         corr_dict_era['corr_lin'][era - 1] = corr(y_era_true, y_era_pred_lin)\n",
    "#         corr_dict_era['corr_lgb'][era - 1] = corr(y_era_true, y_era_pred_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(corr_dict_era, 'model-0/saved-variables/corr_dict_era.pkl')\n",
    "# joblib.dump(corr_dict_fold, 'model-0/saved-variables/corr_dict_fold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(25, 5))\n",
    "# ax1.axhline(0, color='black', linewidth=1)\n",
    "# ax2.axhline(0, color='black', linewidth=1)\n",
    "# ax3.axhline(0, color='black', linewidth=1)\n",
    "# for e in e_val_min:\n",
    "#     ax1.axvline(e, color='gray', linestyle='--')\n",
    "#     ax2.axvline(e, color='gray', linestyle='--')\n",
    "#     ax3.axvline(e, color='gray', linestyle='--')\n",
    "# ax1.plot(eras, corr_dict_era['corr_one'], color='blue'  , linewidth=1, label='one feature')\n",
    "# ax2.plot(eras, corr_dict_era['corr_lin'], color='orange', linewidth=1, label='linear ensemble')\n",
    "# ax3.plot(eras, corr_dict_era['corr_lgb'], color='green' , linewidth=1, label='forest ensemble')\n",
    "# ax1.set_ylim(-0.05, 0.15)\n",
    "# ax2.set_ylim(-0.05, 0.15)\n",
    "# ax3.set_ylim(-0.05, 0.15)\n",
    "# ax1.set_xlabel('era')\n",
    "# ax1.set_ylabel('corr')\n",
    "# ax1.set_title(f'train on many targets and ensemble')\n",
    "# ax1.legend()\n",
    "# ax2.set_xlabel('era')\n",
    "# ax2.set_ylabel('corr')\n",
    "# ax2.set_title(f'train on many targets and ensemble')\n",
    "# ax2.legend()\n",
    "# ax3.set_xlabel('era')\n",
    "# ax3.set_ylabel('corr')\n",
    "# ax3.set_title(f'train on many targets and ensemble')\n",
    "# ax3.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_df_fold = pd.DataFrame(corr_dict_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV 3: Feature neutralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# df = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df[ERA] = df[ERA].astype('int32')\n",
    "# df = df.fillna(0.5)\n",
    "\n",
    "# X_COLS = FEAT_L\n",
    "\n",
    "# X = df[X_COLS]\n",
    "# y = df[Y_TRUE]\n",
    "# e = df[ERA]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# spl = TimeSeriesSplitGroups()\n",
    "\n",
    "# model = LGBMRegressor(**params)\n",
    "# model = EraSubsampler(model)\n",
    "# model = FeatureNeutralizer(model, n_features=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_numbers = [5, 10, 25, 50, 100]\n",
    "# alphas = np.arange(0, 1, 0.01)\n",
    "\n",
    "# i = 0\n",
    "# for trn, val in spl.split(X, y, e):\n",
    "#     i += 1\n",
    "#     print(f'in iteration {i}/5 of CV')\n",
    "#     X_val = X.iloc[val]\n",
    "#     X_trn = X.iloc[trn]\n",
    "#     y_val = y.iloc[val]\n",
    "#     y_trn = y.iloc[trn]\n",
    "#     e_val = e.iloc[val]\n",
    "#     e_trn = e.iloc[trn]\n",
    "\n",
    "#     print('\\ttraining model')\n",
    "#     model.fit(X_trn, y_trn, eras=e_trn)\n",
    "#     joblib.dump(model, f'model-0/saved-variables/model_neut_{i}')\n",
    "\n",
    "#     print('\\tcomputing y_pred')\n",
    "#     model.compute_y_pred(X_val)\n",
    "#     joblib.dump(model, f'model-0/saved-variables/model_neut_{i}')\n",
    "\n",
    "#     print('\\tcomputing corrs')\n",
    "#     for n_feats in feat_numbers:\n",
    "#         model.set_params(n_features=n_feats)\n",
    "#         model.compute_y_linr(X_val, groups=e_val)\n",
    "#         corrs = [(corr(y_val, model.y_pred - a * model.y_linr, rank_a=e_val), a) for a in alphas]\n",
    "#         c, a = sorted(corrs, reverse=True)[0]\n",
    "#         print(f'\\t\\tCV_fold = {i}, n_features = {n_feats}, a = {a:.2f}, c = {c:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "# }\n",
    "\n",
    "# model = LGBMRegressor(**params)\n",
    "# model = EraSubsampler(model)\n",
    "\n",
    "# df_trn = pd.read_parquet('data/train.parquet', columns=COLUMNS)\n",
    "# df_trn[ERA] = df_trn[ERA].astype('int32')\n",
    "\n",
    "# model.fit(df_trn[X_COLS], df_trn[Y_TRUE], eras=df_trn[ERA])\n",
    "\n",
    "# joblib.dump(model, f'model-0/saved-variables/lgbm_{now_dt()}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_liv = pd.read_parquet(f'data/live_{round}.parquet', columns=COLUMNS)\n",
    "\n",
    "# model = joblib.load('model-0/saved-variables/lgbm_2022-08-07-09-37.pkl')\n",
    "# df_liv[Y_PRED] = model.predict(df_liv[X_COLS])\n",
    "# df_liv[Y_RANK] = df_liv[Y_PRED].rank(pct=True)\n",
    "# df_liv[Y_RANK].to_csv(f'model-0/predictions/lgbm_live_predictions_{round}_{now_dt()}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d202d1b6c0c7975210c24a4862339e0f7f90d66cb89735f264f6c4d5c4350e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
