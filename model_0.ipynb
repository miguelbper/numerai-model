{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerai API\n",
    "from numerapi import NumerAPI\n",
    "\n",
    "# data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# stats\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# other\n",
    "import gc\n",
    "import json\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "# save variables\n",
    "import joblib\n",
    "\n",
    "# my utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 12:50:04,266 INFO numerapi.utils: target file already exists\n",
      "2022-08-01 12:50:04,270 INFO numerapi.utils: download complete\n",
      "2022-08-01 12:50:05,421 INFO numerapi.utils: target file already exists\n",
      "2022-08-01 12:50:05,422 INFO numerapi.utils: download complete\n",
      "2022-08-01 12:50:06,495 INFO numerapi.utils: target file already exists\n",
      "2022-08-01 12:50:06,496 INFO numerapi.utils: download complete\n",
      "2022-08-01 12:50:07,620 INFO numerapi.utils: target file already exists\n",
      "2022-08-01 12:50:07,621 INFO numerapi.utils: download complete\n"
     ]
    }
   ],
   "source": [
    "napi = NumerAPI()\n",
    "round = napi.get_current_round()\n",
    "era = round + 695\n",
    "\n",
    "napi.download_dataset('v4/features.json', '../data/features.json')\n",
    "napi.download_dataset('v4/train_int8.parquet', '../data/train.parquet')\n",
    "napi.download_dataset('v4/validation_int8.parquet', '../data/validation.parquet')\n",
    "napi.download_dataset('v4/live_int8.parquet', f'../data/live_{round}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EraSubsampler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EraSubsampler(BaseEstimator):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y, eras):\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "        e0 = eras.min()\n",
    "        e1 = eras.max() + 1\n",
    "        self.model = [deepcopy(self.estimator) for i in range(4)]\n",
    "        for i in trange(4):\n",
    "            self.model[i].fit(X[eras.isin(np.arange(e0 + i, e1, 4))], \n",
    "                              y[eras.isin(np.arange(e0 + i, e1, 4))])\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = check_array(X, accept_sparse=True)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        y_pred = 0\n",
    "        for i in trange(4):\n",
    "            y_pred += self.model[i].predict(X)\n",
    "        y_pred /= 4\n",
    "        return y_pred\n",
    "\n",
    "    # TODO: make score function be the numerai_score (depends on groups)\n",
    "    def score(self, X, y):\n",
    "        return r2_score(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_pct(x):\n",
    "    return x.rank(pct=True, method='first')\n",
    "\n",
    "def numerai_score(y_true, y_pred, groups=None):\n",
    "    if groups is None:\n",
    "        r_pred = rank_pct(y_pred)\n",
    "    else:\n",
    "        r_pred = y_pred.groupby(groups).apply(rank_pct)\n",
    "    return np.corrcoef(y_true, r_pred)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM with Era Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "params = {\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'num_leaves': 2**5,\n",
    "    'colsample_bytree': 0.1,\n",
    "    'device': 'gpu',\n",
    "}\n",
    "\n",
    "gbm = LGBMRegressor(**params)\n",
    "gbm = EraSubsampler(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [06:31<00:00, 97.98s/it] \n",
      "100%|██████████| 4/4 [06:06<00:00, 91.58s/it] \n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "df_trn = pd.read_parquet('../data/train.parquet')\n",
    "df_trn[ERA] = df_trn[ERA].astype('int32')\n",
    "# df_trn = df_trn[df_trn[ERA] <= 8]\n",
    "\n",
    "gbm.fit(df_trn[X_COLS], df_trn[Y_TRUE], df_trn[ERA])\n",
    "\n",
    "df_trn[Y_PRED] = gbm.predict(df_trn[X_COLS])\n",
    "corr_trn = numerai_score(df_trn[Y_TRUE], df_trn[Y_PRED], df_trn[ERA])\n",
    "del df_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [04:49<00:00, 72.26s/it] \n"
     ]
    }
   ],
   "source": [
    "# validation set\n",
    "df_val = pd.read_parquet('../data/validation.parquet')\n",
    "df_val = df_val[df_val[DATA]=='validation']\n",
    "df_val[ERA] = df_val[ERA].astype('int32')\n",
    "# df_val = df_val[df_val[ERA] <= 575 + 7]\n",
    "\n",
    "df_val[Y_PRED] = gbm.predict(df_val[X_COLS])\n",
    "corr_val = numerai_score(df_val[Y_TRUE], df_val[Y_PRED], df_val[ERA])\n",
    "del df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  6.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# live set\n",
    "df_liv = pd.read_parquet(f'../data/live_{round}.parquet')\n",
    "\n",
    "df_liv[Y_TRUE] = gbm.predict(df_liv[X_COLS])\n",
    "df_liv[Y_RANK] = df_liv[Y_TRUE].rank(pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variables\n",
    "now = datetime.now().strftime('%Y%m%d%H%M')\n",
    "name = 'lgbm'\n",
    "joblib.dump(gbm, f'saved-variables/{name}_{now}.pkl')\n",
    "df_liv[Y_RANK].to_csv(f'predictions/{name}_live_predictions_{round}_{now}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV: use era as feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'num_leaves': 2**5,\n",
    "    'colsample_bytree': 0.1,\n",
    "    'device': 'gpu',\n",
    "}\n",
    "\n",
    "n_splits = 4\n",
    "\n",
    "gbm = LGBMRegressor(**params)\n",
    "gbm = EraSubsampler(gbm)\n",
    "\n",
    "cvs = TimeSeriesSplitGroups(n_splits)\n",
    "\n",
    "df = pd.read_parquet('../data/train.parquet')\n",
    "df[ERA] = df[ERA].astype('int32')\n",
    "df = df[df[ERA] <= 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i = 1, trn = [0,...,16305], val = [16306,...,18687]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:48<00:00, 12.05s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_no_era = 0.36079655944615097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:47<00:00, 11.98s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_w_era = 0.36262564442282963\n",
      "\n",
      "i = 2, trn = [0,...,13885], val = [13886,...,16305]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:46<00:00, 11.54s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_no_era = 0.38057547350969884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:45<00:00, 11.28s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_w_era = 0.380543721705323\n",
      "\n",
      "i = 3, trn = [0,...,11470], val = [11471,...,13885]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:44<00:00, 11.06s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_no_era = 0.43559394307229965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:44<00:00, 11.11s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_w_era = 0.4364399565739641\n",
      "\n",
      "i = 4, trn = [0,...,9064], val = [9065,...,11470]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:42<00:00, 10.54s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_no_era = 0.4939229274004983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:42<00:00, 10.53s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_w_era = 0.49386004056235594\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10668/850778074.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mcorrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Miguel\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\Miguel\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All arrays must be of the same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "ind = cvs.split(df[X_ERAS], groups=df[ERA])\n",
    "\n",
    "corrs = {'corr_no_era': [], 'corr_w_era': []}\n",
    "\n",
    "i = 1\n",
    "\n",
    "for ind_trn, ind_val in cvs.split(df[X_ERAS], groups=df[ERA]):\n",
    "    # define train and validation sets for this fold\n",
    "    trn_0 = ind_trn[0]\n",
    "    trn_1 = ind_trn[-1] + 1\n",
    "    val_0 = ind_val[0]\n",
    "    val_1 = ind_val[-1] + 1\n",
    "    \n",
    "    df_trn = df[trn_0:trn_1].copy()\n",
    "    df_val = df[val_0:val_1].copy()\n",
    "\n",
    "    print(f'\\ni = {i}, trn = [{trn_0},...,{trn_1-1}], val = [{val_0},...,{val_1-1}]')\n",
    "\n",
    "    # fit and compute oos corr without using era as a feature\n",
    "    gbm.fit(df_trn[X_COLS], df_trn[Y_TRUE], df_trn[ERA])\n",
    "    df_val[Y_PRED] = gbm.predict(df_val[X_COLS])\n",
    "    corr = numerai_score(df_val[Y_TRUE], df_val[Y_PRED], df_val[ERA])\n",
    "    corrs['corr_no_era'].append(corr)\n",
    "    print(f'corr_no_era = {corr}')\n",
    "\n",
    "    # fit and compute oos corr using era as a feature\n",
    "    gbm.fit(df_trn[X_ERAS], df_trn[Y_TRUE], df_trn[ERA])\n",
    "    df_val[Y_PRED] = gbm.predict(df_val[X_ERAS])\n",
    "    corr = numerai_score(df_val[Y_TRUE], df_val[Y_PRED], df_val[ERA])\n",
    "    corrs['corr_w_era'].append(corr)\n",
    "    print(f'corr_w_era = {corr}')\n",
    "\n",
    "    i += 1\n",
    "\n",
    "corrs = pd.DataFrame(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost / LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameters for LightGBM\n",
    "# params = {\n",
    "#     'n_estimators': 2000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 5,\n",
    "#     'max_leaves': 2**5,\n",
    "#     'colsample_bytree': 0.1,\n",
    "#     'device': 'gpu',\n",
    "#     # 'gpu_id': 0,\n",
    "#     # 'tree_method': 'gpu_hist',\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn = pd.read_parquet('v4/train_int8.parquet', columns=COLUMNS)\n",
    "# df_trn[ERA] = df_trn[ERA].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "\n",
    "# # xgb = EraSubsampler(XGBRegressor(**params))\n",
    "# xgb = EraSubsampler(LGBMRegressor(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train\n",
    "# xgb.fit(df_trn[X_COLS], df_trn[Y_TRUE], df_trn[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict and score on training set\n",
    "# df_trn[Y_PRED] = xgb.predict(df_trn[X_COLS])\n",
    "# df_trn[Y_RANK] = df_trn[Y_PRED].rank(pct=True)\n",
    "# ns_trn = numerai_score(df_trn[Y_TRUE], df_trn[Y_PRED], df_trn[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define validation set\n",
    "# df_val = pd.read_parquet('v4/validation_int8.parquet', columns=COLUMNS)\n",
    "# df_val = df_val[df_val[DATA].isin(['validation'])]\n",
    "# df_val[ERA] = df_val[ERA].astype('int32')\n",
    "# # df_val = df_val[df_val[ERA] <= 575 + 20]\n",
    "\n",
    "# # predict and score on validation set\n",
    "# df_val[Y_PRED] = xgb.predict(df_val[X_COLS])\n",
    "# df_val[Y_RANK] = df_val[Y_PRED].rank(pct=True)\n",
    "# ns_val = numerai_score(df_val[Y_TRUE], df_val[Y_PRED], df_val[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define live set\n",
    "# df_liv = pd.read_parquet(f'v4/live_int8_{round}.parquet', columns=COLUMNS)\n",
    "\n",
    "# # predict on validation set\n",
    "# df_liv[Y_PRED] = xgb.predict(df_liv[X_COLS])\n",
    "# df_liv[Y_RANK] = df_liv[Y_PRED].rank(pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save variables\n",
    "# now = datetime.now().strftime('%Y%m%d%H%M')\n",
    "# joblib.dump(xgb, f'saved-variables/lgbm_{now}.pkl')\n",
    "# df_val[Y_RANK].to_csv(f'predictions/lgbm_validation_predictions_{round}_{now}.csv')\n",
    "# df_liv[Y_RANK].to_csv(f'predictions/lgbm_live_predictions_{round}_{now}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'estimator__' + k: [v] for k, v in params.items()}\n",
    "# xgbGS = GridSearchCV(EraSubsampler(XGBRegressor()), param_grid)\n",
    "\n",
    "# xgbGS.fit(df_trn[X_COLS], df_trn[Y_TRUE], groups=df_trn[ERA], **{'eras': df_trn[ERA]})\n",
    "\n",
    "# bst = xgbGS.best_estimator_\n",
    "\n",
    "# df_trn['bst_' + Y_PRED] = bst.predict(df_trn[X_COLS])\n",
    "# df_trn['bst_' + Y_RANK] = df_trn['bst_' + Y_PRED].rank(pct=True)\n",
    "# ns_bst = numerai_score(df_trn[Y_TRUE], df_trn['bst_' + Y_PRED], df_trn[ERA])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature neutralization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Neutralizer(BaseEstimator):\n",
    "#     # in this class: X = [era | features]\n",
    "#     def __init__(self, estimator, n_features, alpha):\n",
    "#         self.estimator = estimator\n",
    "#         self.n_features = n_features\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def fit(self, X, y, **fit_params):\n",
    "#         X, y = check_X_y(X, y, accept_sparse=True)\n",
    "#         X = X[X_COLS]\n",
    "#         self.estimator.fit(X, y, **fit_params)\n",
    "#         self.is_fitted_ = True\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         X = check_array(X, accept_sparse=True)\n",
    "#         check_is_fitted(self, 'is_fitted_')\n",
    "#         eras = X[ERA]\n",
    "#         X = X[X_COLS]\n",
    "#         y_pred = self.estimator.predict(X)\n",
    "#         if self.alpha == 0:\n",
    "#             return y_pred\n",
    "#         y_linr = 0\n",
    "#         y_neut = y_pred - self.alpha * y_linr\n",
    "#         return y_neut\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         X = check_array(X, accept_sparse=True)\n",
    "#         check_is_fitted(self, 'is_fitted_')\n",
    "#         return r2_score(y, self.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d202d1b6c0c7975210c24a4862339e0f7f90d66cb89735f264f6c4d5c4350e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
