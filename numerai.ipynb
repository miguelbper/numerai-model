{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerai API\n",
    "from numerapi import NumerAPI\n",
    "\n",
    "# data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# stats\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# other\n",
    "import gc\n",
    "import json\n",
    "from tqdm import trange\n",
    "from itertools import product\n",
    "import functools\n",
    "import random\n",
    "from timeit import default_timer\n",
    "import re\n",
    "import time\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from varname import nameof\n",
    "from datetime import datetime\n",
    "\n",
    "# save variables\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# my utils\n",
    "from utils import rank_pct, numerai_score, exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "napi = NumerAPI()\n",
    "round = napi.get_current_round()\n",
    "\n",
    "# filenames = napi.list_datasets()\n",
    "\n",
    "napi.download_dataset('v4/live.parquet', f'v4/live_{round}.parquet')\n",
    "napi.download_dataset('v4/live_int8.parquet', f'v4/live_int8_{round}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "- feature sets: `all`, `small`, `medium`, `v2_equivalent_features`, `v3_equivalent_features`, `fncv3_features`\n",
    "- feature groups: `features_all[0:210]`, `features_all[210:420]`, `features_all[420:630]`, `features_all[630:840]`, `features_all[840:1050]`, `features_all[1050:1191]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v4/FEATURES.json', 'r') as f:\n",
    "    FEATURE_METADATA = json.load(f)\n",
    "del f\n",
    "\n",
    "FEATURES_L = list(FEATURE_METADATA['feature_stats'].keys())\n",
    "FEATURES_M = FEATURE_METADATA['feature_sets']['medium']\n",
    "FEATURES_S = FEATURE_METADATA['feature_sets']['small']\n",
    "FEATURES_2 = FEATURE_METADATA['feature_sets']['v2_equivalent_features']\n",
    "FEATURES_3 = FEATURE_METADATA['feature_sets']['v3_equivalent_features']\n",
    "FEATURES_N = FEATURE_METADATA['feature_sets']['fncv3_features']\n",
    "\n",
    "ERA = 'era'\n",
    "DATA = 'data_type'\n",
    "Y_TRUE = 'target_nomi_v4_20'\n",
    "Y_PRED = 'target_prediction'\n",
    "Y_RANK = 'prediction' \n",
    "\n",
    "X_COLS = FEATURES_L\n",
    "COLUMNS = [ERA, DATA] + X_COLS + [Y_TRUE]\n",
    "\n",
    "df_feature_metadata = pd.DataFrame(FEATURE_METADATA['feature_stats'])\n",
    "df_feature_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn = pd.read_parquet('v4/train_int8.parquet', columns=COLUMNS)\n",
    "# df_trn[ERA] = df_trn[ERA].astype('int32')\n",
    "# df_trn.info(memory_usage='deep')\n",
    "# df_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val = pd.read_parquet('v4/validation_int8.parquet', columns=COLUMNS)\n",
    "# df_val[ERA] = df_val[ERA].astype('int32')\n",
    "# df_val.info(memory_usage='deep')\n",
    "# df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_liv = pd.read_parquet('v4/live_int8_{round}.parquet', columns=COLUMNS)\n",
    "# df_liv.info(memory_usage='deep')\n",
    "# df_liv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of examples as a function of the era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_trn = df.groupby(ERA).size().index.values\n",
    "# y_trn = df_trn.groupby(ERA).size().values\n",
    "# x_val = df_val[df_val[DATA]=='validation'].groupby(ERA).size().index.values\n",
    "# y_val = df_val[df_val[DATA]=='validation'].groupby(ERA).size().values\n",
    "# x_tst = df_val[df_val[DATA]=='test'].groupby(ERA).size().index.values\n",
    "# y_tst = df_val[df_val[DATA]=='test'].groupby(ERA).size().values\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x_trn, y_trn, label='train')\n",
    "# ax.plot(x_val, y_val, label='validation')\n",
    "# ax.plot(x_tst, y_tst, label='test')\n",
    "# ax.set_xlabel(ERA)\n",
    "# ax.set_ylabel('number of examples')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_val\n",
    "# del df_liv\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_correlations = df_trn[df_trn[ERA]==1][FEATURES].corr()\n",
    "# plt.figure(figsize = (8,8))\n",
    "# plt.imshow(feature_correlations)\n",
    "# for a in [210, 420, 630, 840, 1050]:\n",
    "#     plt.axvline(a, color='orange')\n",
    "#     plt.axhline(a, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of feature with target as a function of the era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def corrs_with_target(era):\n",
    "#     return np.corrcoef(df_trn[df_trn[ERA]==era][[TARGET] + FEATURES].T)[0, 1:]\n",
    "\n",
    "# eras = df_trn[ERA].unique()\n",
    "# t_corrs = np.array([corrs_with_target(era) for era in eras])\n",
    "# t_corrs = pd.DataFrame(t_corrs)\n",
    "# t_corrs.rename(columns = dict(enumerate(FEATURES)), inplace=True)\n",
    "# t_corrs.insert(0, ERA, eras)\n",
    "# joblib.dump(t_corrs, 'saved-variables/t_corrs.pkl')\n",
    "# t_corrs = joblib.load('saved-variables/t_corrs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = t_corrs[ERA]\n",
    "# y = t_corrs['feature_untidy_withdrawn_bargeman']\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, y)\n",
    "# ax.set_xlabel(ERA)\n",
    "# ax.set_ylabel('correlation with target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for testing models\n",
    "\n",
    "Performance metrics:\n",
    "\n",
    "- correlation\n",
    "- rank-correlation / spearman-correlation\n",
    "- `sklearn.metrics.r2_score`\n",
    "- `sklearn.metrics.mean_squared_error`\n",
    "\n",
    "Models worth trying at first\n",
    "\n",
    "- `sklearn.linear_model.LinearRegression()`\n",
    "- `sklearn.linear_model.LogisticRegression()` (This doesn't work, it's only for classification - but some example uses it?)\n",
    "- `sklearn.linear_model.SGDRegressor()` (Stochastic Gradient Descent regressor)\n",
    "- `sklearn.linear_model.Lasso()`\n",
    "- `sklearn.linear_model.ElasticNet()`\n",
    "- `sklearn.linear_model.Ridge()`\n",
    "- `sklearn.svm.SVR(kernel='rbf')` (Support Vector Machine / Regression)\n",
    "- `sklearn.svm.SVR(kernel='linear')`\n",
    "- `lightgbm.LGBMRegressor()`\n",
    "- `xgboost.XGBRegressor()`\n",
    "\n",
    "Ensembles\n",
    "\n",
    "- `sklearn.ensemble.RandomForestRegressor()`\n",
    "- `sklearn.ensemble.ExtraTreesRegressor()`\n",
    "- `sklearn.ensemble.BaggingRegressor()`\n",
    "- `sklearn.ensemble.AdaBoostRegressor()`\n",
    "- `sklearn.ensemble.GradientBoostingRegressor()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EraSubsampler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EraSubsampler(BaseEstimator):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y, eras):\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "        e0 = eras.min()\n",
    "        e1 = eras.max() + 1\n",
    "        self.model = [deepcopy(self.estimator) for i in range(4)]\n",
    "        for i in trange(4):\n",
    "            self.model[i].fit(X[eras.isin(np.arange(e0 + i, e1, 4))], \n",
    "                              y[eras.isin(np.arange(e0 + i, e1, 4))])\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = check_array(X, accept_sparse=True)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        y_pred = 0\n",
    "        for i in trange(4):\n",
    "            y_pred += self.model[i].predict(X)\n",
    "        y_pred /= 4\n",
    "        return y_pred\n",
    "\n",
    "    # TODO: make score function be the numerai_score (depends on groups)\n",
    "    def score(self, X, y):\n",
    "        return r2_score(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "X_COLS = FEATURES_S\n",
    "COLUMNS = [ERA, DATA] + X_COLS + [Y_TRUE]\n",
    "df_trn = pd.read_parquet('v4/train_int8.parquet', columns=COLUMNS)\n",
    "df_trn[ERA] = df_trn[ERA].astype('int32')\n",
    "df_trn = df_trn[df_trn[ERA] <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "params = {\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'max_leaves': 2**5,\n",
    "    'colsample_bytree': 0.1,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "}\n",
    "xgb = EraSubsampler(XGBRegressor(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "xgb.fit(df_trn[X_COLS], df_trn[Y_TRUE], df_trn[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and score on training set\n",
    "df_trn[Y_PRED] = xgb.predict(df_trn[X_COLS])\n",
    "df_trn[Y_RANK] = df_trn[Y_PRED].rank(pct=True)\n",
    "ns_trn = numerai_score(df_trn[Y_TRUE], df_trn[Y_PRED], df_trn[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation set\n",
    "df_val = pd.read_parquet('v4/validation_int8.parquet', columns=COLUMNS)\n",
    "df_val[ERA] = df_val[ERA].astype('int32')\n",
    "# df_val = df_val[df_val[ERA] <= 575 + 20]\n",
    "\n",
    "# predict and score on validation set\n",
    "df_val[Y_PRED] = xgb.predict(df_val[X_COLS])\n",
    "df_val[Y_RANK] = df_val[Y_PRED].rank(pct=True)\n",
    "ns_val = numerai_score(df_val[Y_TRUE], df_val[Y_PRED], df_val[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define live set\n",
    "df_liv = pd.read_parquet(f'v4/live_int8_{round}.parquet', columns=COLUMNS)\n",
    "\n",
    "# predict on validation set\n",
    "df_liv[Y_PRED] = xgb.predict(df_liv[X_COLS])\n",
    "df_liv[Y_RANK] = df_liv[Y_PRED].rank(pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variables\n",
    "now = datetime.now().strftime('%Y%m%d%H%M')\n",
    "joblib.dump(xgb, f'saved-variables/xgb_{now}.pkl')\n",
    "df_val[Y_RANK].to_csv(f'predictions/validation_predictions_{round}_{now}.csv')\n",
    "df_liv[Y_RANK].to_csv(f'predictions/live_predictions_{round}_{now}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'estimator__' + k: [v] for k, v in params.items()}\n",
    "# xgbGS = GridSearchCV(EraSubsampler(XGBRegressor()), param_grid)\n",
    "\n",
    "# xgbGS.fit(df_trn[X_COLS], df_trn[Y_TRUE], groups=df_trn[ERA], **{'eras': df_trn[ERA]})\n",
    "\n",
    "# bst = xgbGS.best_estimator_\n",
    "\n",
    "# df_trn['bst_' + Y_PRED] = bst.predict(df_trn[X_COLS])\n",
    "# df_trn['bst_' + Y_RANK] = df_trn['bst_' + Y_PRED].rank(pct=True)\n",
    "# ns_bst = numerai_score(df_trn[Y_TRUE], df_trn['bst_' + Y_PRED], df_trn[ERA])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d202d1b6c0c7975210c24a4862339e0f7f90d66cb89735f264f6c4d5c4350e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
