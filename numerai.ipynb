{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerai API\n",
    "from numerapi import NumerAPI\n",
    "\n",
    "# data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# stats\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# other\n",
    "import gc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import functools\n",
    "import random\n",
    "from timeit import default_timer\n",
    "import re\n",
    "import time\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "# save variables\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# my utils\n",
    "from utils import rank_pct, numerai_score, exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 01:43:39,562 INFO numerapi.utils: target file already exists\n",
      "2022-07-30 01:43:39,565 INFO numerapi.utils: download complete\n",
      "2022-07-30 01:43:40,596 INFO numerapi.utils: target file already exists\n",
      "2022-07-30 01:43:40,599 INFO numerapi.utils: download complete\n"
     ]
    }
   ],
   "source": [
    "napi = NumerAPI()\n",
    "round = napi.get_current_round()\n",
    "\n",
    "# filenames = napi.list_datasets()\n",
    "\n",
    "napi.download_dataset('v4/live.parquet', f'v4/live_{round}.parquet')\n",
    "napi.download_dataset('v4/live_int8.parquet', f'v4/live_int8_{round}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and columns to read\n",
    "\n",
    "- feature sets: `all`, `small`, `medium`, `v2_equivalent_features`, `v3_equivalent_features`, `fncv3_features`\n",
    "- feature groups: `features_all[0:210]`, `features_all[210:420]`, `features_all[420:630]`, `features_all[630:840]`, `features_all[840:1050]`, `features_all[1050:1191]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_honoured_observational_balaamite</th>\n",
       "      <th>feature_polaroid_vadose_quinze</th>\n",
       "      <th>feature_untidy_withdrawn_bargeman</th>\n",
       "      <th>feature_genuine_kyphotic_trehala</th>\n",
       "      <th>feature_unenthralled_sportful_schoolhouse</th>\n",
       "      <th>feature_divulsive_explanatory_ideologue</th>\n",
       "      <th>feature_ichthyotic_roofed_yeshiva</th>\n",
       "      <th>feature_waggly_outlandish_carbonisation</th>\n",
       "      <th>feature_floriated_amish_sprite</th>\n",
       "      <th>feature_iconoclastic_parietal_agonist</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_circumspective_daughterly_brubeck</th>\n",
       "      <th>feature_mimetic_sprawly_flue</th>\n",
       "      <th>feature_inductile_umbrian_wallah</th>\n",
       "      <th>feature_ineloquent_bihari_brougham</th>\n",
       "      <th>feature_shakespearean_alpha_constituent</th>\n",
       "      <th>feature_marxian_plated_refrigeration</th>\n",
       "      <th>feature_amative_irresponsive_flattie</th>\n",
       "      <th>feature_intermissive_coronal_reinsertion</th>\n",
       "      <th>feature_dwarfish_isochronal_amateur</th>\n",
       "      <th>feature_polyphyletic_unplumed_pandiculation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>legacy_uniqueness</th>\n",
       "      <td>0.177814</td>\n",
       "      <td>0.241351</td>\n",
       "      <td>0.659092</td>\n",
       "      <td>0.234994</td>\n",
       "      <td>0.471051</td>\n",
       "      <td>0.608926</td>\n",
       "      <td>0.220884</td>\n",
       "      <td>0.671897</td>\n",
       "      <td>0.878900</td>\n",
       "      <td>0.174533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777375</td>\n",
       "      <td>0.788337</td>\n",
       "      <td>0.798391</td>\n",
       "      <td>0.777608</td>\n",
       "      <td>0.781240</td>\n",
       "      <td>0.801397</td>\n",
       "      <td>0.812955</td>\n",
       "      <td>0.824060</td>\n",
       "      <td>0.793313</td>\n",
       "      <td>0.806686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr_w_target_nomi_20_mean</th>\n",
       "      <td>-0.000796</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.000619</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>-0.000623</td>\n",
       "      <td>-0.003439</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.001762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005185</td>\n",
       "      <td>-0.005866</td>\n",
       "      <td>-0.006759</td>\n",
       "      <td>-0.005132</td>\n",
       "      <td>-0.005950</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>-0.002635</td>\n",
       "      <td>-0.003977</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>-0.002001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr_w_target_nomi_20_sharpe</th>\n",
       "      <td>-0.078689</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>-0.067669</td>\n",
       "      <td>0.127591</td>\n",
       "      <td>0.065213</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>-0.084122</td>\n",
       "      <td>-0.323518</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-0.161949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216388</td>\n",
       "      <td>-0.252259</td>\n",
       "      <td>-0.310896</td>\n",
       "      <td>-0.219533</td>\n",
       "      <td>-0.247247</td>\n",
       "      <td>-0.094700</td>\n",
       "      <td>-0.130897</td>\n",
       "      <td>-0.205489</td>\n",
       "      <td>-0.077834</td>\n",
       "      <td>-0.095218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr_w_target_nomi_20_reversals</th>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr_w_target_nomi_20_autocorr</th>\n",
       "      <td>-0.013665</td>\n",
       "      <td>0.110503</td>\n",
       "      <td>0.036986</td>\n",
       "      <td>0.149465</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>-0.066927</td>\n",
       "      <td>0.093560</td>\n",
       "      <td>-0.041187</td>\n",
       "      <td>-0.127240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>-0.009986</td>\n",
       "      <td>0.013370</td>\n",
       "      <td>-0.011183</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>-0.007453</td>\n",
       "      <td>-0.020823</td>\n",
       "      <td>-0.012853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr_w_target_nomi_20_arl</th>\n",
       "      <td>3.650350</td>\n",
       "      <td>3.456954</td>\n",
       "      <td>2.916201</td>\n",
       "      <td>3.984733</td>\n",
       "      <td>3.702128</td>\n",
       "      <td>3.411765</td>\n",
       "      <td>3.262500</td>\n",
       "      <td>3.575342</td>\n",
       "      <td>3.017341</td>\n",
       "      <td>3.984733</td>\n",
       "      <td>...</td>\n",
       "      <td>3.782609</td>\n",
       "      <td>3.434211</td>\n",
       "      <td>3.650350</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>3.755396</td>\n",
       "      <td>3.434211</td>\n",
       "      <td>3.503356</td>\n",
       "      <td>3.503356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 1191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature_honoured_observational_balaamite  \\\n",
       "legacy_uniqueness                                                         0.177814   \n",
       "spearman_corr_w_target_nomi_20_mean                                      -0.000796   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                    -0.078689   \n",
       "spearman_corr_w_target_nomi_20_reversals                                  0.000074   \n",
       "spearman_corr_w_target_nomi_20_autocorr                                  -0.013665   \n",
       "spearman_corr_w_target_nomi_20_arl                                        3.650350   \n",
       "\n",
       "                                          feature_polaroid_vadose_quinze  \\\n",
       "legacy_uniqueness                                               0.241351   \n",
       "spearman_corr_w_target_nomi_20_mean                             0.000199   \n",
       "spearman_corr_w_target_nomi_20_sharpe                           0.020379   \n",
       "spearman_corr_w_target_nomi_20_reversals                        0.000082   \n",
       "spearman_corr_w_target_nomi_20_autocorr                         0.110503   \n",
       "spearman_corr_w_target_nomi_20_arl                              3.456954   \n",
       "\n",
       "                                          feature_untidy_withdrawn_bargeman  \\\n",
       "legacy_uniqueness                                                  0.659092   \n",
       "spearman_corr_w_target_nomi_20_mean                               -0.000619   \n",
       "spearman_corr_w_target_nomi_20_sharpe                             -0.067669   \n",
       "spearman_corr_w_target_nomi_20_reversals                           0.000067   \n",
       "spearman_corr_w_target_nomi_20_autocorr                            0.036986   \n",
       "spearman_corr_w_target_nomi_20_arl                                 2.916201   \n",
       "\n",
       "                                          feature_genuine_kyphotic_trehala  \\\n",
       "legacy_uniqueness                                                 0.234994   \n",
       "spearman_corr_w_target_nomi_20_mean                               0.001724   \n",
       "spearman_corr_w_target_nomi_20_sharpe                             0.127591   \n",
       "spearman_corr_w_target_nomi_20_reversals                          0.000095   \n",
       "spearman_corr_w_target_nomi_20_autocorr                           0.149465   \n",
       "spearman_corr_w_target_nomi_20_arl                                3.984733   \n",
       "\n",
       "                                          feature_unenthralled_sportful_schoolhouse  \\\n",
       "legacy_uniqueness                                                          0.471051   \n",
       "spearman_corr_w_target_nomi_20_mean                                        0.000661   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                      0.065213   \n",
       "spearman_corr_w_target_nomi_20_reversals                                   0.000072   \n",
       "spearman_corr_w_target_nomi_20_autocorr                                    0.014508   \n",
       "spearman_corr_w_target_nomi_20_arl                                         3.702128   \n",
       "\n",
       "                                          feature_divulsive_explanatory_ideologue  \\\n",
       "legacy_uniqueness                                                        0.608926   \n",
       "spearman_corr_w_target_nomi_20_mean                                     -0.001529   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                   -0.173158   \n",
       "spearman_corr_w_target_nomi_20_reversals                                 0.000058   \n",
       "spearman_corr_w_target_nomi_20_autocorr                                  0.023341   \n",
       "spearman_corr_w_target_nomi_20_arl                                       3.411765   \n",
       "\n",
       "                                          feature_ichthyotic_roofed_yeshiva  \\\n",
       "legacy_uniqueness                                                  0.220884   \n",
       "spearman_corr_w_target_nomi_20_mean                               -0.000623   \n",
       "spearman_corr_w_target_nomi_20_sharpe                             -0.084122   \n",
       "spearman_corr_w_target_nomi_20_reversals                           0.000052   \n",
       "spearman_corr_w_target_nomi_20_autocorr                           -0.066927   \n",
       "spearman_corr_w_target_nomi_20_arl                                 3.262500   \n",
       "\n",
       "                                          feature_waggly_outlandish_carbonisation  \\\n",
       "legacy_uniqueness                                                        0.671897   \n",
       "spearman_corr_w_target_nomi_20_mean                                     -0.003439   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                   -0.323518   \n",
       "spearman_corr_w_target_nomi_20_reversals                                 0.000076   \n",
       "spearman_corr_w_target_nomi_20_autocorr                                  0.093560   \n",
       "spearman_corr_w_target_nomi_20_arl                                       3.575342   \n",
       "\n",
       "                                          feature_floriated_amish_sprite  \\\n",
       "legacy_uniqueness                                               0.878900   \n",
       "spearman_corr_w_target_nomi_20_mean                             0.000012   \n",
       "spearman_corr_w_target_nomi_20_sharpe                           0.001100   \n",
       "spearman_corr_w_target_nomi_20_reversals                        0.000105   \n",
       "spearman_corr_w_target_nomi_20_autocorr                        -0.041187   \n",
       "spearman_corr_w_target_nomi_20_arl                              3.017341   \n",
       "\n",
       "                                          feature_iconoclastic_parietal_agonist  \\\n",
       "legacy_uniqueness                                                      0.174533   \n",
       "spearman_corr_w_target_nomi_20_mean                                   -0.001762   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                 -0.161949   \n",
       "spearman_corr_w_target_nomi_20_reversals                               0.000086   \n",
       "spearman_corr_w_target_nomi_20_autocorr                               -0.127240   \n",
       "spearman_corr_w_target_nomi_20_arl                                     3.984733   \n",
       "\n",
       "                                          ...  \\\n",
       "legacy_uniqueness                         ...   \n",
       "spearman_corr_w_target_nomi_20_mean       ...   \n",
       "spearman_corr_w_target_nomi_20_sharpe     ...   \n",
       "spearman_corr_w_target_nomi_20_reversals  ...   \n",
       "spearman_corr_w_target_nomi_20_autocorr   ...   \n",
       "spearman_corr_w_target_nomi_20_arl        ...   \n",
       "\n",
       "                                          feature_circumspective_daughterly_brubeck  \\\n",
       "legacy_uniqueness                                                          0.777375   \n",
       "spearman_corr_w_target_nomi_20_mean                                       -0.005185   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                     -0.216388   \n",
       "spearman_corr_w_target_nomi_20_reversals                                   0.000385   \n",
       "spearman_corr_w_target_nomi_20_autocorr                                    0.007061   \n",
       "spearman_corr_w_target_nomi_20_arl                                         3.782609   \n",
       "\n",
       "                                          feature_mimetic_sprawly_flue  \\\n",
       "legacy_uniqueness                                             0.788337   \n",
       "spearman_corr_w_target_nomi_20_mean                          -0.005866   \n",
       "spearman_corr_w_target_nomi_20_sharpe                        -0.252259   \n",
       "spearman_corr_w_target_nomi_20_reversals                      0.000365   \n",
       "spearman_corr_w_target_nomi_20_autocorr                       0.014354   \n",
       "spearman_corr_w_target_nomi_20_arl                            3.434211   \n",
       "\n",
       "                                          feature_inductile_umbrian_wallah  \\\n",
       "legacy_uniqueness                                                 0.798391   \n",
       "spearman_corr_w_target_nomi_20_mean                              -0.006759   \n",
       "spearman_corr_w_target_nomi_20_sharpe                            -0.310896   \n",
       "spearman_corr_w_target_nomi_20_reversals                          0.000324   \n",
       "spearman_corr_w_target_nomi_20_autocorr                           0.006205   \n",
       "spearman_corr_w_target_nomi_20_arl                                3.650350   \n",
       "\n",
       "                                          feature_ineloquent_bihari_brougham  \\\n",
       "legacy_uniqueness                                                   0.777608   \n",
       "spearman_corr_w_target_nomi_20_mean                                -0.005132   \n",
       "spearman_corr_w_target_nomi_20_sharpe                              -0.219533   \n",
       "spearman_corr_w_target_nomi_20_reversals                            0.000386   \n",
       "spearman_corr_w_target_nomi_20_autocorr                            -0.009986   \n",
       "spearman_corr_w_target_nomi_20_arl                                  3.625000   \n",
       "\n",
       "                                          feature_shakespearean_alpha_constituent  \\\n",
       "legacy_uniqueness                                                        0.781240   \n",
       "spearman_corr_w_target_nomi_20_mean                                     -0.005950   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                   -0.247247   \n",
       "spearman_corr_w_target_nomi_20_reversals                                 0.000393   \n",
       "spearman_corr_w_target_nomi_20_autocorr                                  0.013370   \n",
       "spearman_corr_w_target_nomi_20_arl                                       3.625000   \n",
       "\n",
       "                                          feature_marxian_plated_refrigeration  \\\n",
       "legacy_uniqueness                                                     0.801397   \n",
       "spearman_corr_w_target_nomi_20_mean                                  -0.001996   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                -0.094700   \n",
       "spearman_corr_w_target_nomi_20_reversals                              0.000324   \n",
       "spearman_corr_w_target_nomi_20_autocorr                              -0.011183   \n",
       "spearman_corr_w_target_nomi_20_arl                                    3.866667   \n",
       "\n",
       "                                          feature_amative_irresponsive_flattie  \\\n",
       "legacy_uniqueness                                                     0.812955   \n",
       "spearman_corr_w_target_nomi_20_mean                                  -0.002635   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                -0.130897   \n",
       "spearman_corr_w_target_nomi_20_reversals                              0.000306   \n",
       "spearman_corr_w_target_nomi_20_autocorr                              -0.002933   \n",
       "spearman_corr_w_target_nomi_20_arl                                    3.755396   \n",
       "\n",
       "                                          feature_intermissive_coronal_reinsertion  \\\n",
       "legacy_uniqueness                                                         0.824060   \n",
       "spearman_corr_w_target_nomi_20_mean                                      -0.003977   \n",
       "spearman_corr_w_target_nomi_20_sharpe                                    -0.205489   \n",
       "spearman_corr_w_target_nomi_20_reversals                                  0.000288   \n",
       "spearman_corr_w_target_nomi_20_autocorr                                  -0.007453   \n",
       "spearman_corr_w_target_nomi_20_arl                                        3.434211   \n",
       "\n",
       "                                          feature_dwarfish_isochronal_amateur  \\\n",
       "legacy_uniqueness                                                    0.793313   \n",
       "spearman_corr_w_target_nomi_20_mean                                 -0.001651   \n",
       "spearman_corr_w_target_nomi_20_sharpe                               -0.077834   \n",
       "spearman_corr_w_target_nomi_20_reversals                             0.000342   \n",
       "spearman_corr_w_target_nomi_20_autocorr                             -0.020823   \n",
       "spearman_corr_w_target_nomi_20_arl                                   3.503356   \n",
       "\n",
       "                                          feature_polyphyletic_unplumed_pandiculation  \n",
       "legacy_uniqueness                                                            0.806686  \n",
       "spearman_corr_w_target_nomi_20_mean                                         -0.002001  \n",
       "spearman_corr_w_target_nomi_20_sharpe                                       -0.095218  \n",
       "spearman_corr_w_target_nomi_20_reversals                                     0.000313  \n",
       "spearman_corr_w_target_nomi_20_autocorr                                     -0.012853  \n",
       "spearman_corr_w_target_nomi_20_arl                                           3.503356  \n",
       "\n",
       "[6 rows x 1191 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('v4/FEATURES.json', 'r') as f:\n",
    "    FEATURE_METADATA = json.load(f)\n",
    "del f\n",
    "\n",
    "FEATURES_L = list(FEATURE_METADATA['feature_stats'].keys())\n",
    "FEATURES_M = FEATURE_METADATA['feature_sets']['medium']\n",
    "FEATURES_S = FEATURE_METADATA['feature_sets']['small']\n",
    "FEATURES_2 = FEATURE_METADATA['feature_sets']['v2_equivalent_features']\n",
    "FEATURES_3 = FEATURE_METADATA['feature_sets']['v3_equivalent_features']\n",
    "FEATURES_N = FEATURE_METADATA['feature_sets']['fncv3_features']\n",
    "\n",
    "ERA = 'era'\n",
    "DATA = 'data_type'\n",
    "TARGET = 'target_nomi_v4_20'\n",
    "PRED = 'prediction'\n",
    "\n",
    "FEATURES = FEATURES_L\n",
    "N_FEATURES = len(FEATURES)\n",
    "COLUMNS = [ERA, DATA] + FEATURES + [TARGET]\n",
    "\n",
    "df_feature_metadata = pd.DataFrame(FEATURE_METADATA['feature_stats'])\n",
    "df_feature_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn = pd.read_parquet('v4/train_int8.parquet', columns=COLUMNS)\n",
    "# df_trn[ERA] = df_trn[ERA].astype('int32')\n",
    "# df_trn.info(memory_usage='deep')\n",
    "# df_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val = pd.read_parquet('v4/validation_int8.parquet', columns=COLUMNS)\n",
    "# df_val[ERA] = df_val[ERA].astype('int32')\n",
    "# df_val.info(memory_usage='deep')\n",
    "# df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_liv = pd.read_parquet('v4/live_int8_{round}.parquet', columns=COLUMNS)\n",
    "# df_liv.info(memory_usage='deep')\n",
    "# df_liv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of examples as a function of the era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_trn = df.groupby(ERA).size().index.values\n",
    "# y_trn = df_trn.groupby(ERA).size().values\n",
    "# x_val = df_val[df_val[DATA]=='validation'].groupby(ERA).size().index.values\n",
    "# y_val = df_val[df_val[DATA]=='validation'].groupby(ERA).size().values\n",
    "# x_tst = df_val[df_val[DATA]=='test'].groupby(ERA).size().index.values\n",
    "# y_tst = df_val[df_val[DATA]=='test'].groupby(ERA).size().values\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x_trn, y_trn, label='train')\n",
    "# ax.plot(x_val, y_val, label='validation')\n",
    "# ax.plot(x_tst, y_tst, label='test')\n",
    "# ax.set_xlabel(ERA)\n",
    "# ax.set_ylabel('number of examples')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_val\n",
    "# del df_liv\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_correlations = df_trn[df_trn[ERA]==1][FEATURES].corr()\n",
    "# plt.figure(figsize = (8,8))\n",
    "# plt.imshow(feature_correlations)\n",
    "# for a in [210, 420, 630, 840, 1050]:\n",
    "#     plt.axvline(a, color='orange')\n",
    "#     plt.axhline(a, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of feature with target as a function of the era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def corrs_with_target(era):\n",
    "#     return np.corrcoef(df_trn[df_trn[ERA]==era][[TARGET] + FEATURES].T)[0, 1:]\n",
    "\n",
    "# eras = df_trn[ERA].unique()\n",
    "# t_corrs = np.array([corrs_with_target(era) for era in eras])\n",
    "# t_corrs = pd.DataFrame(t_corrs)\n",
    "# t_corrs.rename(columns = dict(enumerate(FEATURES)), inplace=True)\n",
    "# t_corrs.insert(0, ERA, eras)\n",
    "# joblib.dump(t_corrs, 'saved-variables/t_corrs.pkl')\n",
    "# t_corrs = joblib.load('saved-variables/t_corrs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = t_corrs[ERA]\n",
    "# y = t_corrs['feature_untidy_withdrawn_bargeman']\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, y)\n",
    "# ax.set_xlabel(ERA)\n",
    "# ax.set_ylabel('correlation with target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for testing models\n",
    "\n",
    "In this section we test many models without caring about hyperparameters (i.e., just use the defaults for each model). The goal is to identify which models look the most promising. We want to consider the time to train, as well as\n",
    "\n",
    "Performance metrics:\n",
    "\n",
    "- correlation\n",
    "- rank-correlation / spearman-correlation\n",
    "- `sklearn.metrics.r2_score`\n",
    "- `sklearn.metrics.mean_squared_error`\n",
    "\n",
    "Models worth trying at first\n",
    "\n",
    "- `sklearn.linear_model.LinearRegression()`\n",
    "- `sklearn.linear_model.LogisticRegression()` (This doesn't work, it's only for classification - but some example uses it?)\n",
    "- `sklearn.linear_model.SGDRegressor()` (Stochastic Gradient Descent regressor)\n",
    "- `sklearn.linear_model.Lasso()`\n",
    "- `sklearn.linear_model.ElasticNet()`\n",
    "- `sklearn.linear_model.Ridge()`\n",
    "- `sklearn.svm.SVR(kernel='rbf')` (Support Vector Machine / Regression)\n",
    "- `sklearn.svm.SVR(kernel='linear')`\n",
    "- `lightgbm.LGBMRegressor()`\n",
    "- `xgboost.XGBRegressor()`\n",
    "\n",
    "Ensembles\n",
    "\n",
    "- `sklearn.ensemble.RandomForestRegressor()`\n",
    "- `sklearn.ensemble.ExtraTreesRegressor()`\n",
    "- `sklearn.ensemble.BaggingRegressor()`\n",
    "- `sklearn.ensemble.AdaBoostRegressor()`\n",
    "- `sklearn.ensemble.GradientBoostingRegressor()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BlockRegressor:\n",
    "#     def __init__(self, base_regressor):\n",
    "#         self.base_regressor = base_regressor\n",
    "#         self.times = np.zeros((4, 6))\n",
    "#         self.base_models = np.array([[base_regressor() for j in range(6)] for i in range(4)])\n",
    "\n",
    "#     def fit(self, df):\n",
    "#         for i, j in product(range(4), range(6)):\n",
    "#             X = X_block(df, i, j)\n",
    "#             y = y_rows(df, i)\n",
    "#             t0 = default_timer()\n",
    "#             self.base_models[i, j].fit(X, y)\n",
    "#             self.times[i, j] = default_timer() - t0\n",
    "\n",
    "#     def predict(self, df):\n",
    "#         y_mean = 0\n",
    "#         for i, j in product(range(4), range(6)):\n",
    "#             y_mean += self.base_models[i, j].predict(X_cols(df, j))\n",
    "#         y_mean /= 24\n",
    "#         return y_mean\n",
    "\n",
    "#     def score(self, df, y_pred = None):\n",
    "#         y_true = df[TARGET].to_numpy()\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         return r2_score(y_true, y_pred)\n",
    "\n",
    "#     def corr(self, df, y_pred = None):\n",
    "#         y_true = df[TARGET].to_numpy()\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         return np.corrcoef(y_true, y_pred)[0,1]\n",
    "\n",
    "#     def r_corr(self, df, y_pred = None):\n",
    "#         y_true = df[TARGET].to_numpy()\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         return spearmanr(y_true, y_pred)[0]\n",
    "\n",
    "#     def n_corr(self, df, y_pred = None):\n",
    "#         y_true = df[TARGET].to_numpy()\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         y_eras = pd.DataFrame({ERA: df.era, 'y': y_pred})\n",
    "#         r_pred = y_eras.groupby(y_eras.era).apply(lambda x: x.rank(pct=True, method='first'))['y'].to_numpy()\n",
    "#         return np.corrcoef(y_true, r_pred)[0,1]\n",
    "\n",
    "#     def mse(self, df, y_pred = None):\n",
    "#         y_true = df[TARGET].to_numpy()\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "#     def to_dataframe(self, df, y_pred = None):\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "        \n",
    "#         df_dict = {'model': [], 'i': [], 'j': [], 'time': [], 'r2': [], 'corr': [], 'r_corr': [], 'n_corr': [], 'mse': []}\n",
    "\n",
    "#         df_dict['model'].append(string_from_class(self.base_regressor))\n",
    "#         df_dict['i'].append(-1)\n",
    "#         df_dict['j'].append(-1)\n",
    "#         df_dict['time'].append(np.sum([self.times[i, j] for i, j in product(range(4), range(6))]))\n",
    "#         df_dict['r2'].append(self.score(df, y_pred))\n",
    "#         df_dict['corr'].append(self.corr(df, y_pred))\n",
    "#         df_dict['r_corr'].append(self.r_corr(df, y_pred))\n",
    "#         df_dict['n_corr'].append(self.n_corr(df, y_pred))\n",
    "#         df_dict['mse'].append(self.mse(df, y_pred))\n",
    "        \n",
    "#         for i, j in product(range(4), range(6)):\n",
    "#             model = self.base_models[i, j]\n",
    "#             X = X_block(df, i, j)\n",
    "#             y_true = y_rows(df, i)\n",
    "#             y_pred = model.predict(X)\n",
    "#             t = self.times[i, j]\n",
    "#             e0 = df[ERA][0]\n",
    "#             e1 = df[ERA][-1]\n",
    "#             y_eras = pd.DataFrame({ERA: df[df.era.isin(era_subsample(e0, e1, i))].era, 'y': y_pred})\n",
    "#             r_pred = y_eras.groupby(y_eras.era).apply(lambda x: x.rank(pct=True, method='first'))['y'].to_numpy()\n",
    "\n",
    "#             df_dict['model'].append(string_from_class(self.base_regressor))\n",
    "#             df_dict['i'].append(i)\n",
    "#             df_dict['j'].append(j)\n",
    "#             df_dict['time'].append(t)\n",
    "#             df_dict['r2'].append(r2_score(y_true, y_pred))\n",
    "#             df_dict['corr'].append(np.corrcoef(y_true, y_pred)[0,1])\n",
    "#             df_dict['r_corr'].append(spearmanr(y_true, y_pred)[0])\n",
    "#             df_dict['n_corr'].append(np.corrcoef(y_true, r_pred)[0,1])\n",
    "#             df_dict['mse'].append(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "#         return pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Era Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EraEnsemble:\n",
    "#     def __init__(self, base_regressor):\n",
    "#         self.base_regressor = base_regressor\n",
    "#         self.base_models = [base_regressor() for i in range(4)]\n",
    "#         self.time_fit = 0\n",
    "#         self.time_pred = 0\n",
    "\n",
    "#     def fit(self, df):\n",
    "#         print(f'\\nTraining model: {string_from_class(self.base_regressor)}')\n",
    "#         t0 = default_timer()\n",
    "#         for i in range(4):\n",
    "#             print(f'\\tTraining for era_subsample = {i+1}/4')\n",
    "#             X = X_(df, eras = i)\n",
    "#             y = y_(df, eras = i)\n",
    "#             self.base_models[i].fit(X, y)\n",
    "#         self.time_fit = default_timer() - t0\n",
    "\n",
    "#     def predict(self, df):\n",
    "#         print(f'Predicting with model: {string_from_class(self.base_regressor)}')\n",
    "#         t0 = default_timer()\n",
    "#         y_mean = 0\n",
    "#         for i in range(4):\n",
    "#             print(f'\\tPredicting with era_subsample = {i+1}/4')\n",
    "#             y_mean += self.base_models[i].predict(X_(df))\n",
    "#         y_mean /= 4\n",
    "#         self.time_pred = default_timer() - t0\n",
    "#         return y_mean\n",
    "\n",
    "#     def score(self, df, y_pred = None):\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         return r2_score(y_(df), y_pred)\n",
    "\n",
    "#     def corr(self, df, y_pred = None):\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         return np.corrcoef(y_(df), y_pred)[0,1]\n",
    "\n",
    "#     def r_corr(self, df, y_pred = None):\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         return spearmanr(y_(df), y_pred)[0]\n",
    "\n",
    "#     def mse(self, df, y_pred = None):\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         return mean_squared_error(y_(df), y_pred)\n",
    "\n",
    "#     def n_corr(self, df, y_pred = None):\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         y_eras = pd.DataFrame({ERA: df.era, 'y': y_pred})\n",
    "#         r_pred = y_eras.groupby(y_eras.era).apply(rank_pct)['y']\n",
    "#         return np.corrcoef(y_(df), r_pred)[0,1]\n",
    "\n",
    "#     def scores(self, df, y_pred = None):\n",
    "#         if y_pred is None:\n",
    "#             y_pred = self.predict(df)\n",
    "#         d = dict()\n",
    "#         d['model'] = string_from_class(self.base_regressor)\n",
    "#         d['time_fit'] = self.time_fit\n",
    "#         d['time_pred'] = self.time_pred\n",
    "#         d['r2'] = self.score(df, y_pred)\n",
    "#         d['corr'] = self.corr(df, y_pred)\n",
    "#         d['r_corr'] = self.r_corr(df, y_pred)\n",
    "#         d['n_corr'] = self.n_corr(df, y_pred)\n",
    "#         d['mse'] = self.mse(df, y_pred)\n",
    "#         return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with manual era subsampling and neutralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "FEATURES = FEATURES_S\n",
    "COLUMNS = [ERA, DATA] + FEATURES + [TARGET]\n",
    "df_trn = pd.read_parquet('v4/train_int8.parquet', columns=COLUMNS)\n",
    "df_trn[ERA] = df_trn[ERA].astype('int32')\n",
    "df_trn = df_trn[df_trn[ERA] <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 4 models\n",
    "params = {\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'max_leaves': 2**5,\n",
    "    'colsample_bytree': 0.1,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "}\n",
    "xgb1 = XGBRegressor(**params)\n",
    "xgb2 = XGBRegressor(**params)\n",
    "xgb3 = XGBRegressor(**params)\n",
    "xgb4 = XGBRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model 1\n",
      "training model 2\n",
      "training model 3\n",
      "training model 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define training data for each era subsample\n",
    "# train each of the 4 models on each era subsample\n",
    "\n",
    "e0 = df_trn[ERA].min()\n",
    "e1 = df_trn[ERA].max() + 1\n",
    "\n",
    "print('training model 1')\n",
    "dfs = df_trn[df_trn[ERA].isin(np.arange(e0 + 0, e1, 4))]\n",
    "xgb1.fit(dfs[FEATURES], dfs[TARGET])\n",
    "\n",
    "print('training model 2')\n",
    "dfs = df_trn[df_trn[ERA].isin(np.arange(e0 + 1, e1, 4))]\n",
    "xgb2.fit(dfs[FEATURES], dfs[TARGET])\n",
    "\n",
    "print('training model 3')\n",
    "dfs = df_trn[df_trn[ERA].isin(np.arange(e0 + 2, e1, 4))]\n",
    "xgb3.fit(dfs[FEATURES], dfs[TARGET])\n",
    "\n",
    "print('training model 4')\n",
    "dfs = df_trn[df_trn[ERA].isin(np.arange(e0 + 3, e1, 4))]\n",
    "xgb4.fit(dfs[FEATURES], dfs[TARGET])\n",
    "\n",
    "del dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(xgb1, 'saved-variables/xgb1.pkl')\n",
    "# joblib.dump(xgb2, 'saved-variables/xgb2.pkl')\n",
    "# joblib.dump(xgb3, 'saved-variables/xgb3.pkl')\n",
    "# joblib.dump(xgb4, 'saved-variables/xgb4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and score model on training set\n",
    "y1 = xgb1.predict(df_trn[FEATURES])\n",
    "y2 = xgb2.predict(df_trn[FEATURES])\n",
    "y3 = xgb3.predict(df_trn[FEATURES])\n",
    "y4 = xgb4.predict(df_trn[FEATURES])\n",
    "df_trn['y_pred'] = (y1 + y2 + y3 + y4) / 4\n",
    "df_trn['prediction'] = df_trn['y_pred'].rank(pct=True)\n",
    "ns_trn = numerai_score(df_trn[TARGET], df_trn['y_pred'], df_trn[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature exposures\n",
    "# exps = pd.Series({f: exposure(df_trn[f], df_trn['y_pred']) for f in FEATURES})\n",
    "# exps = exps.sort_values(ascending=False)\n",
    "# exps.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature neutralization\n",
    "# RISKY_FEATURES = exps.head(5).index # example they use the 50 riskiest feats\n",
    "# neut = LinearRegression()\n",
    "# neut.fit(df_trn[RISKY_FEATURES], df_trn['y_pred'])\n",
    "# df_trn['y_neut'] = df_trn['y_pred'] - neut.predict(df_trn[RISKY_FEATURES])\n",
    "# ns_trn_neut = numerai_score(df_trn[TARGET], df_trn['y_neut'], df_trn[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_trn\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict and score on validation set\n",
    "\n",
    "# ## define validation set\n",
    "# df_val = pd.read_parquet('v4/validation_int8.parquet', columns=COLUMNS)\n",
    "# df_val = df_val[df_val[DATA]=='validation']\n",
    "# df_val[ERA] = df_val[ERA].astype('int32')\n",
    "\n",
    "# ## predict on validation set\n",
    "# y1 = xgb1.predict(df_val[FEATURES])\n",
    "# y2 = xgb2.predict(df_val[FEATURES])\n",
    "# y3 = xgb3.predict(df_val[FEATURES])\n",
    "# y4 = xgb4.predict(df_val[FEATURES])\n",
    "# df_val['y_pred'] = (y1 + y2 + y3 + y4) / 4\n",
    "# df_val['prediction'] = df_val['y_pred'].rank(pct=True)\n",
    "# df_val['prediction'].to_csv(f'predictions/validation_predictions_{round}.csv')\n",
    "# ns_val = numerai_score(df_val[TARGET], df_val['y_pred'], df_val[ERA])\n",
    "\n",
    "# ## neutralize\n",
    "# df_val['y_neut'] = df_val['y_pred'] - neut.predict(df_val[RISKY_FEATURES])\n",
    "# ns_val_neut = numerai_score(df_val[TARGET], df_val['y_neut'], df_val[ERA])\n",
    "# df_val['y_neut_rank'] = df_val['y_neut'].rank(pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict on live data\n",
    "\n",
    "# ## load live data\n",
    "# df_liv = pd.read_parquet(f'v4/live_int8_{round}.parquet', columns=COLUMNS)\n",
    "\n",
    "# ## predict on live data\n",
    "# y1 = xgb1.predict(df_liv[FEATURES])\n",
    "# y2 = xgb2.predict(df_liv[FEATURES])\n",
    "# y3 = xgb3.predict(df_liv[FEATURES])\n",
    "# y4 = xgb4.predict(df_liv[FEATURES])\n",
    "# df_liv['y_pred'] = (y1 + y2 + y3 + y4) / 4\n",
    "# df_liv['prediction'] = df_liv['y_pred'].rank(pct='True')\n",
    "# df_liv['prediction'].to_csv(f'predictions/live_predictions_{round}.csv')\n",
    "\n",
    "# ## neutralize\n",
    "# df_liv['y_neut'] = df_liv['y_pred'] - neut.predict(df_liv[RISKY_FEATURES])\n",
    "# df_liv['y_neut_rank'] = df_liv['y_neut'].rank(pct='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EraSubsampler class (remake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EraSubsampler:\n",
    "    def __init__(self, base_estimator):\n",
    "        self.model = [deepcopy(base_estimator) for i in range(4)]\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.model[0].get_params()\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for i in range(4):\n",
    "            self.model[i].set_params(**params)\n",
    "\n",
    "    # TODO: O meu problema com isto Ã©:\n",
    "    # X, y, groups ocupam 3GB fora da function call\n",
    "    # eu chamo a funÃ§Ã£o. Durante o tempo que a funÃ§Ã£o estÃ¡ a executar\n",
    "    # eles ocupam mais 3GB dentro da funÃ§Ã£o...\n",
    "    # quem me dera poder passar por apontador lol\n",
    "    # -> simplesmente experimentar. pode ser que funcione\n",
    "    # na minha experiencia, ele nÃ£o gastou mais memÃ³ria quando foi para\n",
    "    # dentro do corpo da funÃ§Ã£o\n",
    "    def fit(self, X, y, groups):\n",
    "        e0 = groups.min()\n",
    "        e1 = groups.max() + 1\n",
    "        for i in range(4):\n",
    "            self.model[i].fit(X[groups.isin(np.arange(e0 + i, e1, 4))], \n",
    "                              y[groups.isin(np.arange(e0 + i, e1, 4))])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = 0\n",
    "        for i in range(4):\n",
    "            y_pred += self.model[i].predict(X)\n",
    "        y_pred /= 4\n",
    "        return y_pred\n",
    "\n",
    "    # TODO: idea: have the score function be the numerai_score.\n",
    "    # The dificulty is that this depends on groups\n",
    "    def score(self, X, y):\n",
    "        return r2_score(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'max_leaves': 2**5,\n",
    "    'colsample_bytree': 0.1,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "}\n",
    "base_estimator = XGBRegressor(**params)\n",
    "model = EraSubsampler(base_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = FEATURES_S\n",
    "COLUMNS = [ERA, DATA] + FEATURES + [TARGET]\n",
    "df = pd.read_parquet('v4/train_int8.parquet', columns=COLUMNS)\n",
    "df[ERA] = df[ERA].astype('int32')\n",
    "df = df[df[ERA] <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df[FEATURES], df[TARGET], df[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[PRED] = model.predict(df[FEATURES])\n",
    "ns_class = numerai_score(df[TARGET], df[PRED], df[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model 1\n",
      "training model 2\n",
      "training model 3\n",
      "training model 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=0, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=5, max_leaves=32, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=2000,\n",
       "             n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1 = XGBRegressor(**params)\n",
    "xgb2 = XGBRegressor(**params)\n",
    "xgb3 = XGBRegressor(**params)\n",
    "xgb4 = XGBRegressor(**params)\n",
    "\n",
    "e0 = df[ERA].min()\n",
    "e1 = df[ERA].max() + 1\n",
    "\n",
    "print('training model 1')\n",
    "dfs = df[df[ERA].isin(np.arange(e0 + 0, e1, 4))]\n",
    "xgb1.fit(dfs[FEATURES], dfs[TARGET])\n",
    "\n",
    "print('training model 2')\n",
    "dfs = df[df[ERA].isin(np.arange(e0 + 1, e1, 4))]\n",
    "xgb2.fit(dfs[FEATURES], dfs[TARGET])\n",
    "\n",
    "print('training model 3')\n",
    "dfs = df[df[ERA].isin(np.arange(e0 + 2, e1, 4))]\n",
    "xgb3.fit(dfs[FEATURES], dfs[TARGET])\n",
    "\n",
    "print('training model 4')\n",
    "dfs = df[df[ERA].isin(np.arange(e0 + 3, e1, 4))]\n",
    "xgb4.fit(dfs[FEATURES], dfs[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'groupby'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16104/797597264.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdy1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdy2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdy3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdy4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mns_trn_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerai_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mERA\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Miguel\\Documents\\GitHub\\numerai\\utils.py\u001b[0m in \u001b[0;36mnumerai_score\u001b[1;34m(y_true, y_pred, groups)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mr_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank_pct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mr_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank_pct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'groupby'"
     ]
    }
   ],
   "source": [
    "dy1 = xgb1.predict(df[FEATURES]) #- model.model[0].predict(df[FEATURES])\n",
    "dy2 = xgb2.predict(df[FEATURES]) #- model.model[1].predict(df[FEATURES])\n",
    "dy3 = xgb3.predict(df[FEATURES]) #- model.model[2].predict(df[FEATURES])\n",
    "dy4 = xgb4.predict(df[FEATURES]) #- model.model[3].predict(df[FEATURES])\n",
    "\n",
    "dy = (dy1 + dy2 + dy3 + dy4) / 4\n",
    "ns_trn_2 = numerai_score(df[TARGET], dy, df[ERA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy0 = model.model[0].predict(df[FEATURES])\n",
    "# yy1 = model.model[1].predict(df[FEATURES])\n",
    "# yy2 = model.model[2].predict(df[FEATURES])\n",
    "# yy3 = model.model[3].predict(df[FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(1 + 3, 20 + 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model[0] == model.model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model[0] is model.model[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d202d1b6c0c7975210c24a4862339e0f7f90d66cb89735f264f6c4d5c4350e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
